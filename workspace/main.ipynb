{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS imports\n",
    "from os.path import join, realpath, dirname, exists, abspath, isfile, isdir, basename\n",
    "from os import mkdir, name as os_name, getcwd, environ, pathsep, rename, listdir, remove\n",
    "\n",
    "# Mediapipe imports\n",
    "from mediapipe.python.solutions import drawing_utils as du \n",
    "from mediapipe.python.solutions import hands\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "# Tensorflow & Keras imports\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "from tensorflow.python.keras.utils.all_utils import to_categorical\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras import losses, metrics\n",
    "\n",
    "# Data handeling/visualization imports\n",
    "import cv2\n",
    "from cv2 import imread, imshow, imwrite, flip, cvtColor, COLOR_BGR2RGB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "# General data handeling imports\n",
    "from numpy import array, zeros, concatenate, save, load, argmax, expand_dims, append\n",
    "import numpy as np\n",
    "from uuid import uuid1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options & Settings ðŸ’¾\n",
    "MODEL_VERSION = '1'\n",
    "MODEL_ARCH_ID = 'conv1'\n",
    "MODEL_DATA_SET = 'aTOz'\n",
    "MODEL_NORMFUNC = 'n2'\n",
    "MODEL_EXTRACTION_TYPE = 'absolute'\n",
    "MODEL_DATA_PIPELINE = f'{MODEL_EXTRACTION_TYPE}{MODEL_NORMFUNC}'\n",
    "\n",
    "MODEL_NAME = f'v{MODEL_VERSION}_{MODEL_ARCH_ID}_data-{MODEL_DATA_SET}_pipeline-{MODEL_DATA_PIPELINE}'\n",
    "\n",
    "MP_MODEL_COMPLEXITY = 0\n",
    "MP_DETECTION_CONFIDENCE = 0.75\n",
    "MP_TRACKING_CONFIDENCE = 0.75\n",
    "MP_NUM_HANDS = 1\n",
    "\n",
    "SIGNS = [\n",
    "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "\n",
    "ALL_SIGNS = SIGNS.copy()\n",
    "ALL_SIGNS.insert(0, 'none')\n",
    "CLASS_COUNT = len(ALL_SIGNS)\n",
    "\n",
    "# Constants ðŸš§\n",
    "HAND_LANDMARK_COUNT = 21 # https://mediapipe.dev/images/mobile/hand_landmarks.png\n",
    "HAND_POINT_COUNT = 3 # (x, y, z)\n",
    "HAND_LANDMARK_POINTS = HAND_LANDMARK_COUNT * HAND_POINT_COUNT # (x, y, z)\n",
    "\n",
    "# Paths ðŸ“\n",
    "ROOT_DIR = getcwd()\n",
    "MODELS_DIR = join(ROOT_DIR, 'models')\n",
    "MODEL_DIR = join(MODELS_DIR, MODEL_NAME)\n",
    "MP_DIR = join(ROOT_DIR, 'mediapipe')\n",
    "MP_MODEL_PATH = join(MP_DIR, 'models', 'hand_landmarker.task')\n",
    "LOG_DIR = join(MODEL_DIR, 'logs')\n",
    "SAVED_MODEL_PATH = join(MODEL_DIR, 'signs.h5')\n",
    "EXPORTED_MODEL_DIR = join(MODEL_DIR)\n",
    "DATA_DIR = join(ROOT_DIR, 'data')\n",
    "IMAGES_DIR = join(ROOT_DIR, 'images')\n",
    "COLLECTED_IMAGES_DIR = join(IMAGES_DIR, 'collected')\n",
    "ACCEPTED_IMAGES_DIR = join(IMAGES_DIR, 'accepted')\n",
    "REJECTED_IMAGES_DIR = join(IMAGES_DIR, 'rejected')\n",
    "\n",
    "# Global Utils ðŸ“\n",
    "def create_dir(dir_path: str, notify: bool = True):\n",
    "  if not exists(dir_path):\n",
    "    mkdir(dir_path)\n",
    "  else:\n",
    "    if notify: print(f'{dir_path} already exists!')\n",
    "\n",
    "def dir_exists(dir_path: str) -> bool:\n",
    "  return exists(dir_path) and isdir(dir_path)\n",
    "\n",
    "EMPTY_KEYPOINTS = zeros((HAND_LANDMARK_COUNT, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mediapipe Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Tuple\n",
    "from numpy import ndarray\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "  if not results.multi_hand_landmarks: return image\n",
    "  \n",
    "  hand_landmarks = results.multi_hand_landmarks\n",
    "  \n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(119, 252, 3), thickness=1, circle_radius=2), \n",
    "      du.DrawingSpec(color=(119, 252, 3), thickness=1, circle_radius=2)\n",
    "    )\n",
    "    \n",
    "def draw_img_landmarks(image, hand_landmarks):\n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(224,0,0), thickness=32, circle_radius=5), # points\n",
    "      du.DrawingSpec(color=(0,0,224), thickness=32, circle_radius=5) # edges\n",
    "    )\n",
    "\n",
    "def mediapipe_detection(image, hands: hands.Hands):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # COLOR CONVERSION BGR 2 RGB\n",
    "  image.flags.writeable = False                   # Image is no longer writeable\n",
    "  results = hands.process(image)                  # Make prediction\n",
    "  image.flags.writeable = True                    # Image is now writeable\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # COLOR COVERSION RGB 2 BGR\n",
    "  return image, results\n",
    "\n",
    "def get_handedness(results: NamedTuple) -> str:\n",
    "  return MessageToDict(results.multi_handedness[0])['classification'][0]['label']\n",
    "\n",
    "def is_right_hand(results: NamedTuple) -> bool:\n",
    "  return get_handedness(results) == 'Right'\n",
    "\n",
    "def extract_keypoints(results: NamedTuple) -> ndarray:\n",
    "  if not results.multi_hand_landmarks:\n",
    "    return EMPTY_KEYPOINTS\n",
    "  landmarks = MessageToDict(results.multi_hand_landmarks[0])['landmark']\n",
    "\n",
    "  res = []\n",
    "  for landmark in landmarks:\n",
    "    x = landmark['x']\n",
    "    y = landmark['y']\n",
    "    z = landmark['z']\n",
    "    res.append([ x, y, z ])\n",
    "\n",
    "  return array(res)\n",
    "\n",
    "def extract_absolute_keypoints(results: NamedTuple) -> ndarray:\n",
    "  if not results.multi_hand_landmarks:\n",
    "    return EMPTY_KEYPOINTS\n",
    "  landmarks = MessageToDict(results.multi_hand_world_landmarks[0])['landmark']\n",
    "  # landmarks = MessageToDict(results.multi_hand_landmarks[0])['landmark']\n",
    "\n",
    "  res = []\n",
    "  for landmark in landmarks:\n",
    "    x = landmark['x']\n",
    "    y = landmark['y']\n",
    "    z = landmark['z']\n",
    "    res.append([ x, y, z ])\n",
    "\n",
    "  return array(res)\n",
    "\n",
    "\n",
    "def normalize_landmarks(arr: ndarray):\n",
    "  \"\"\"\n",
    "  Transforms array of absolute positions into an array of normalized position within range [-1, 1]\n",
    "\n",
    "  @param `landmarks` an array of landmark positions of shape (21, 3)\n",
    "  @returns transformned NDArray of floats with shape (21, 3)\n",
    "  \"\"\"\n",
    "  # Compute the minimum and maximum values separately for each dimension\n",
    "  min_x, max_x = np.min(arr[:, 0]), np.max(arr[:, 0])\n",
    "  min_y, max_y = np.min(arr[:, 1]), np.max(arr[:, 1])\n",
    "  min_z, max_z = np.min(arr[:, 2]), np.max(arr[:, 2])\n",
    "\n",
    "  # Normalize each dimension separately\n",
    "  arr[:, 0] = (arr[:, 0] - min_x) / (max_x - min_x)\n",
    "  arr[:, 1] = (arr[:, 1] - min_y) / (max_y - min_y)\n",
    "  arr[:, 2] = (arr[:, 2] - min_z) / (max_z - min_z)\n",
    "\n",
    "  return arr\n",
    "\n",
    "def normalize_landmarks2(landmarks: ndarray):\n",
    "  # Compute the centroid\n",
    "  centroid = np.mean(landmarks, axis=0)\n",
    "\n",
    "  # Translate the points to the centroid\n",
    "  translated_landmarks = landmarks - centroid\n",
    "\n",
    "  # Compute the scaling factor\n",
    "  scale_factor = np.max(np.linalg.norm(translated_landmarks, axis=1))\n",
    "\n",
    "  if scale_factor == 0:\n",
    "    return translated_landmarks\n",
    "\n",
    "  # Normalize the points\n",
    "  normalized_landmarks = translated_landmarks / scale_factor\n",
    "\n",
    "  return normalized_landmarks\n",
    "\n",
    "def normalize_landmarks3(landmarks: ndarray) -> ndarray:\n",
    "  # Calculate the minimum and maximum values along each axis\n",
    "  min_values = np.min(landmarks, axis=0)\n",
    "  max_values = np.max(landmarks, axis=0)\n",
    "\n",
    "  # Compute the bounding box dimensions\n",
    "  bounding_box_dimensions = max_values - min_values\n",
    "\n",
    "  # Compute the scaling factor as the maximum dimension of the bounding box\n",
    "  scaling_factor = np.max(bounding_box_dimensions)\n",
    "\n",
    "  # Translate the points so that the minimum values along each axis are at the origin\n",
    "  translated_landmarks = landmarks - min_values\n",
    "\n",
    "  # Scale the translated points to fit inside the 1x1x1 box\n",
    "  scaled_landmarks = translated_landmarks / scaling_factor\n",
    "\n",
    "  # Translate the scaled points so that their origin is at (0, 0, 0)\n",
    "  min_values_scaled = np.min(scaled_landmarks, axis=0)\n",
    "  normalized_landmarks = scaled_landmarks - min_values_scaled\n",
    "\n",
    "  return normalized_landmarks\n",
    "\n",
    "def print_landmark_values(data: ndarray):\n",
    "  print(tabulate(data, headers=[\"x\", \"y\", \"z\"], floatfmt=\".4f\", stralign='right', numalign='decimal'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Collection & Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Camera Check\n",
    "\n",
    "camera_index = 0\n",
    "while True:\n",
    "    cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\n",
    "    if not cap.read()[0]:\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Camera index: {camera_index} - OK\")\n",
    "    cap.release()\n",
    "    camera_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live Image collection\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "selected_sign_index = 0\n",
    "selected_sign = SIGNS[selected_sign_index]\n",
    "selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def select_next_sign():\n",
    "  global selected_sign, selected_sign_index, selected_sign_amount\n",
    "  if selected_sign_index < len(SIGNS) - 1:\n",
    "    selected_sign_index += 1\n",
    "    selected_sign = SIGNS[selected_sign_index]\n",
    "    selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def select_prev_sign():\n",
    "  global selected_sign, selected_sign_index, selected_sign_amount\n",
    "  if selected_sign_index > 0:\n",
    "    selected_sign_index -= 1\n",
    "    selected_sign = SIGNS[selected_sign_index]\n",
    "    selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def capture(results, image):\n",
    "  global selected_sign, selected_sign_amount\n",
    "  if not results.multi_hand_landmarks:\n",
    "    print('no hand available')\n",
    "    return\n",
    "  if not is_right_hand(results):\n",
    "    print('must use right hand')\n",
    "    return\n",
    "\n",
    "  keypoints = extract_absolute_keypoints(results) # absolute\n",
    "  data = normalize_landmarks2(keypoints) # n2\n",
    "\n",
    "  generated_name = f'{selected_sign}_{selected_sign_amount}.{uuid1()}'\n",
    "  data_path = join(DATA_DIR, selected_sign, generated_name)\n",
    "  img_path = join(COLLECTED_IMAGES_DIR, selected_sign, generated_name) + '.jpeg'\n",
    "  save(data_path, data)\n",
    "  imwrite(img_path, image)\n",
    "  print(f'saved {generated_name}')\n",
    "  selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while cap.isOpened():\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    raw_image = image\n",
    "    image = flip(image, 1)\n",
    "    \n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    image, results = mediapipe_detection(image, mp_hands)\n",
    "    draw_landmarks(image, results)\n",
    "    image = cv2.putText(\n",
    "      image, f'{selected_sign} | {selected_sign_amount}',\n",
    "      (32, 32), cv2.FONT_HERSHEY_SIMPLEX, 1, (119, 252, 3), 2, cv2.LINE_AA\n",
    "    )\n",
    "      \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    imshow('signs', image)\n",
    "    \n",
    "    key = cv2.waitKeyEx(1)\n",
    "\n",
    "    if key == 32: # space bar\n",
    "      capture(results, raw_image)\n",
    "\n",
    "    if key == 2555904: # right key\n",
    "      select_next_sign()\n",
    "\n",
    "    if key == 2424832: # left key\n",
    "      select_prev_sign()\n",
    "      \n",
    "    if key == 27:\n",
    "      break\n",
    "  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image selection TODO!\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "\n",
    "def get_image_paths(directory):\n",
    "  image_paths = {}\n",
    "\n",
    "  for subdir in listdir(directory):\n",
    "    subdir_path = join(directory, subdir)\n",
    "    if isdir(subdir_path):\n",
    "      images = []\n",
    "      for filename in listdir(subdir_path):\n",
    "        if any(filename.endswith(ext) for ext in image_extensions):\n",
    "          images.append(join(subdir_path, filename))\n",
    "      image_paths[subdir] = images\n",
    "\n",
    "  return image_paths\n",
    "\n",
    "show_landmarks = True\n",
    "selected_sign = SIGNS[0]\n",
    "selected_img_index = 0\n",
    "\n",
    "images_paths = get_image_paths(COLLECTED_IMAGES_DIR)\n",
    "\n",
    "def create_black_image(width, height):\n",
    "  return zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "def move_files(img_id, img):\n",
    "  pass\n",
    "\n",
    "def accept(img):\n",
    "  pass\n",
    "\n",
    "\n",
    "def reject(img):\n",
    "  pass\n",
    "\n",
    "\n",
    "def move_right():\n",
    "  pass\n",
    "\n",
    "def move_left():\n",
    "  pass\n",
    "\n",
    "def move_up():\n",
    "  pass\n",
    "\n",
    "def move_down():\n",
    "  pass\n",
    "\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while True:\n",
    "\n",
    "    key = cv2.waitKey(0)\n",
    "\n",
    "    can_do_stuff = len(images_paths[selected_sign]) > 0\n",
    "\n",
    "    image = create_black_image(640, 480)\n",
    "\n",
    "    if can_do_stuff:\n",
    "      img_path = images_paths[selected_sign][selected_img_index]\n",
    "      image = cv2.imread(img_path)\n",
    "    else:\n",
    "      # add some text to blck screen\n",
    "      pass\n",
    "\n",
    "    imshow('signs', image)\n",
    "\n",
    "    if key == 101 and can_do_stuff: # e \"accept\"\n",
    "      accept(image)\n",
    "\n",
    "    if key == 113 and can_do_stuff: # q \"reject\"\n",
    "      reject(image)\n",
    "\n",
    "    if key == 2555904 and can_do_stuff: # right key\n",
    "      move_right()\n",
    "\n",
    "    if key == 2424832 and can_do_stuff: # left key\n",
    "      move_left()\n",
    "\n",
    "    if key == 2490368: # up arrow key\n",
    "      move_up()\n",
    "\n",
    "    if key == 2621440: # down arrow key\n",
    "      move_down()\n",
    "      \n",
    "    if key == 27:\n",
    "      break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*generate empty folders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\n",
    "  COLLECTED_IMAGES_DIR,\n",
    "  ACCEPTED_IMAGES_DIR,\n",
    "  REJECTED_IMAGES_DIR,\n",
    "  DATA_DIR\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "  for s in SIGNS:\n",
    "    create_dir(join(directory, s), False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1():\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(21, 3)))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Model Selection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model() -> Sequential:\n",
    "  return conv1()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Loading & Partitioning data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_count():\n",
    "  data_amounts = []\n",
    "  for sign in SIGNS:\n",
    "    data_directory = join(DATA_DIR, sign)\n",
    "    data_amounts.append(len(listdir(join(DATA_DIR, data_directory))))\n",
    "\n",
    "      \n",
    "  return min(data_amounts)\n",
    "\n",
    "COLLECTION_COUNT = get_collection_count()\n",
    "print(f'currently using {COLLECTION_COUNT} data points')\n",
    "\n",
    "# Load Training Data\n",
    "label_map = { label: num for num, label in enumerate(ALL_SIGNS) }\n",
    "sequences, labels = [zeros((COLLECTION_COUNT, 21, 3)), [ 0 for _ in range(COLLECTION_COUNT) ]]\n",
    "\n",
    "\n",
    "for sign in SIGNS:\n",
    "  sign_data_dir = join(DATA_DIR, sign)\n",
    "  for data_file_name in listdir(sign_data_dir)[:COLLECTION_COUNT]:\n",
    "    data_path = join(sign_data_dir, data_file_name)\n",
    "    res = load(data_path)\n",
    "    expanded_res = expand_dims(res, axis=0)\n",
    "    sequences = concatenate((sequences, expanded_res), axis=0)\n",
    "    labels.append(label_map[sign])\n",
    "\n",
    "x = array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "# Testing!\n",
    "x_shape_expected = (CLASS_COUNT * COLLECTION_COUNT, HAND_LANDMARK_COUNT, HAND_POINT_COUNT)\n",
    "x_shape_result = x.shape\n",
    "y_shape_expected = (CLASS_COUNT * COLLECTION_COUNT, CLASS_COUNT)\n",
    "y_shape_result = y.shape\n",
    "if x_shape_result != x_shape_expected:\n",
    "  raise Exception(f'ERROR: expected sequence shape `{x_shape_expected}` != from gotten `{x_shape_result}`')\n",
    "if y_shape_result != y_shape_expected:\n",
    "  raise Exception(f'ERROR: expected labels shape `{y_shape_expected}` != from gotten `{y_shape_result}`')\n",
    "\n",
    "# partitioning train, test, validation data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1 * 1.0)            # 10% test data (1.0 * 0.20 = 0.1 => 10%)\n",
    "x_train, x_val,  y_train, y_val  = train_test_split(x_train, y_train, test_size=0.1/0.9)  # 10% val  data (0.9 * 0.11 = 0.1 => 10%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingByLoss(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, target_loss):\n",
    "    super(EarlyStoppingByLoss, self).__init__()\n",
    "    self.target_loss = target_loss\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    current_loss = logs.get('loss')\n",
    "    if current_loss is not None and current_loss <= self.target_loss:\n",
    "      print(f\"\\nReached target loss {self.target_loss}, stopping training.\")\n",
    "      self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(MODEL_DIR)\n",
    "mkdir(LOG_DIR)\n",
    "\n",
    "tb_callback = TensorBoard(log_dir=LOG_DIR)\n",
    "early_stopping_by_loss = EarlyStoppingByLoss(target_loss=0.1)\n",
    "terminal_log_dir = LOG_DIR.replace('\\\\', '/')\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.compile(\n",
    "  optimizer=optimizers.adam_v2.Adam(learning_rate=0.000025),\n",
    "  loss=losses.CategoricalCrossentropy(),\n",
    "  metrics=[metrics.CategoricalCrossentropy(), metrics.CategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(f'[RUN]: tensorboard --logdir={terminal_log_dir}')\n",
    "\n",
    "model.fit(\n",
    "  x_train, y_train, validation_data=(x_val, y_val),\n",
    "  epochs=10000, batch_size=10,\n",
    "  use_multiprocessing=True, workers=4,\n",
    "  callbacks=[tb_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(SAVED_MODEL_PATH)\n",
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(None, 21, 3)\n",
      "Model: \"sequential_4\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv1d_4 (Conv1D)            (None, 19, 64)            640       \n",
      "_________________________________________________________________\n",
      "max_pooling1d_4 (MaxPooling1 (None, 9, 64)             0         \n",
      "_________________________________________________________________\n",
      "flatten_4 (Flatten)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 128)               73856     \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 27)                3483      \n",
      "=================================================================\n",
      "Total params: 77,979\n",
      "Trainable params: 77,979\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# LOAD MODEL ðŸ’¾\n",
    "model = get_model()\n",
    "model.load_weights(SAVED_MODEL_PATH)\n",
    "print(model.input_shape)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-TRAINING PREDICIONS âš¡\n",
    "res = model.predict(x_test)\n",
    "\n",
    "y_real = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(res)):\n",
    "  y_real.append(argmax(y_test[i]))\n",
    "  y_pred.append(argmax(res[i]))\n",
    "\n",
    "y_real = array(y_real)\n",
    "y_pred = array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX ðŸ˜µ\n",
    "cm = confusion_matrix(y_real, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=ALL_SIGNS)\n",
    "disp.plot(xticks_rotation=75)\n",
    "disp.figure_.set_size_inches(12.8, 7.2)\n",
    "disp.figure_.set_dpi(150)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Predict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "e 0.8710908\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "b 0.9595891\n",
      "b 0.96207017\n",
      "b 0.9639975\n",
      "b 0.9666882\n",
      "b 0.9667118\n",
      "b 0.96220875\n",
      "b 0.9708837\n",
      "b 0.96235013\n",
      "b 0.9735292\n",
      "b 0.97422475\n",
      "b 0.9737163\n",
      "b 0.97658175\n",
      "b 0.9751608\n",
      "b 0.9768759\n",
      "b 0.9757934\n",
      "b 0.97589886\n",
      "b 0.97663236\n",
      "b 0.98189074\n",
      "b 0.98592305\n",
      "c 0.99399143\n",
      "c 0.9949994\n",
      "c 0.96809167\n",
      "c 0.92543447\n",
      "c 0.9776889\n",
      "c 0.9728156\n",
      "c 0.97704566\n",
      "c 0.9848897\n",
      "c 0.9719491\n",
      "c 0.97146016\n",
      "c 0.9711803\n",
      "c 0.9559981\n",
      "c 0.98032856\n",
      "c 0.97504723\n",
      "c 0.9795313\n",
      "c 0.9797977\n",
      "c 0.97432786\n",
      "c 0.9708636\n",
      "o 0.6216553\n",
      "o 0.93636525\n",
      "o 0.42897198\n",
      "c 0.5240399\n",
      "c 0.75052077\n",
      "g 0.5294338\n",
      "g 0.8125892\n",
      "g 0.69612336\n",
      "o 0.45300055\n",
      "o 0.42682177\n",
      "g 0.73386645\n",
      "g 0.6989766\n",
      "g 0.6600176\n",
      "g 0.6366685\n",
      "g 0.64101946\n",
      "g 0.77757\n",
      "g 0.59851813\n",
      "g 0.6250903\n",
      "g 0.6348962\n",
      "g 0.4828834\n",
      "g 0.49943078\n",
      "g 0.47528988\n",
      "g 0.53670925\n",
      "g 0.8842641\n",
      "g 0.72986084\n",
      "g 0.47515824\n",
      "none 0.988165\n",
      "g 0.37435326\n",
      "none 0.988165\n",
      "p 0.5855421\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "b 0.98044086\n",
      "b 0.98348826\n",
      "b 0.9868513\n",
      "b 0.98389083\n",
      "b 0.98213977\n",
      "b 0.98321056\n",
      "b 0.9824785\n",
      "b 0.9838657\n",
      "b 0.98456556\n",
      "b 0.9851753\n",
      "b 0.984371\n",
      "b 0.9838701\n",
      "b 0.9836565\n",
      "b 0.98468983\n",
      "b 0.98342335\n",
      "b 0.98464113\n",
      "b 0.984774\n",
      "c 0.99632615\n",
      "c 0.99608225\n",
      "c 0.987566\n",
      "c 0.984002\n",
      "c 0.9766084\n",
      "c 0.98010546\n",
      "c 0.9733331\n",
      "c 0.9797397\n",
      "c 0.9830161\n",
      "c 0.9792437\n",
      "c 0.9871767\n",
      "c 0.9890008\n",
      "c 0.989408\n",
      "c 0.9873481\n",
      "c 0.98234797\n",
      "o 0.5747132\n",
      "o 0.98619694\n",
      "o 0.77960014\n",
      "o 0.93698895\n",
      "o 0.97759706\n",
      "o 0.9805932\n",
      "o 0.96724975\n",
      "o 0.8305672\n",
      "o 0.5312015\n",
      "o 0.5097308\n",
      "o 0.37471998\n",
      "s 0.6591114\n",
      "s 0.6268646\n",
      "s 0.44740608\n",
      "s 0.4866462\n",
      "o 0.4495033\n",
      "o 0.50903\n",
      "e 0.4103526\n",
      "e 0.35815272\n",
      "o 0.30888817\n",
      "o 0.34715903\n",
      "o 0.35030293\n",
      "o 0.33817312\n",
      "o 0.4470632\n",
      "o 0.38954377\n",
      "o 0.4605359\n",
      "o 0.42401716\n",
      "o 0.5008926\n",
      "o 0.6195146\n",
      "o 0.4714215\n",
      "o 0.45482284\n",
      "o 0.4808673\n",
      "o 0.46578577\n",
      "o 0.37762317\n",
      "o 0.41404426\n",
      "o 0.3926088\n",
      "o 0.40654203\n",
      "o 0.43264833\n",
      "o 0.41514596\n",
      "o 0.40778646\n",
      "o 0.42449394\n",
      "o 0.42688006\n",
      "o 0.5542375\n",
      "o 0.54975694\n",
      "o 0.54814255\n",
      "o 0.39163366\n",
      "o 0.32949308\n",
      "o 0.29691795\n",
      "y 0.31317803\n",
      "y 0.30147952\n",
      "y 0.31530395\n",
      "y 0.37126395\n",
      "y 0.33840358\n",
      "y 0.39933598\n",
      "y 0.35635868\n",
      "y 0.3409973\n",
      "y 0.3619042\n",
      "y 0.29753327\n",
      "y 0.3103602\n",
      "y 0.28261554\n",
      "y 0.2834559\n",
      "y 0.27556667\n",
      "o 0.28755483\n",
      "o 0.29798314\n",
      "y 0.2900311\n",
      "y 0.26899248\n",
      "y 0.28401354\n",
      "y 0.28392324\n",
      "y 0.23643234\n",
      "i 0.2381586\n",
      "o 0.25338548\n",
      "o 0.26077807\n",
      "o 0.27368525\n",
      "o 0.28506884\n",
      "o 0.2874267\n",
      "o 0.3012772\n",
      "o 0.300275\n",
      "o 0.30717233\n",
      "o 0.30819362\n",
      "o 0.38950312\n",
      "o 0.34223402\n",
      "o 0.38142267\n",
      "o 0.34343892\n",
      "o 0.34179196\n",
      "o 0.35242683\n",
      "i 0.3286885\n",
      "o 0.33831704\n",
      "i 0.3154606\n",
      "o 0.3425952\n",
      "i 0.32178742\n",
      "o 0.33234945\n",
      "o 0.30980796\n",
      "o 0.98865974\n",
      "o 0.7191989\n",
      "o 0.79608047\n",
      "o 0.7797088\n",
      "o 0.8210278\n",
      "o 0.9140009\n",
      "o 0.9116477\n",
      "o 0.9167988\n",
      "o 0.92205596\n",
      "o 0.9489003\n",
      "o 0.9529105\n",
      "o 0.95593435\n",
      "o 0.9588334\n",
      "o 0.957198\n",
      "o 0.95801014\n",
      "o 0.95580727\n",
      "o 0.9590394\n",
      "o 0.9602405\n",
      "o 0.959099\n",
      "o 0.95695823\n",
      "o 0.958434\n",
      "o 0.95947605\n",
      "o 0.9610412\n",
      "o 0.96037704\n",
      "o 0.96433985\n",
      "o 0.9633849\n",
      "o 0.9625726\n",
      "o 0.9640438\n",
      "o 0.97184485\n",
      "o 0.9735528\n",
      "o 0.9733054\n",
      "o 0.97079504\n",
      "o 0.79722786\n",
      "o 0.3085961\n",
      "v 0.5693805\n",
      "v 0.6122201\n",
      "v 0.78721803\n",
      "v 0.8514346\n",
      "v 0.8489677\n",
      "v 0.8184011\n",
      "v 0.8185738\n",
      "v 0.8053429\n",
      "v 0.81382287\n",
      "v 0.79285145\n",
      "v 0.81636053\n",
      "v 0.79621154\n",
      "v 0.7947713\n",
      "v 0.79508126\n",
      "v 0.79346013\n",
      "v 0.78963137\n",
      "v 0.7838604\n",
      "v 0.77221656\n",
      "v 0.78221804\n",
      "v 0.80121183\n",
      "v 0.8125234\n",
      "v 0.86459166\n",
      "v 0.8783217\n",
      "v 0.78628445\n",
      "v 0.80187815\n",
      "v 0.7331097\n",
      "v 0.7796927\n",
      "v 0.7820902\n",
      "v 0.78183514\n",
      "v 0.7739381\n",
      "v 0.78372395\n",
      "v 0.79124004\n",
      "v 0.8063554\n",
      "v 0.80649376\n",
      "v 0.79872596\n",
      "v 0.7734568\n",
      "v 0.75899005\n",
      "v 0.76224345\n",
      "v 0.5126859\n",
      "k 0.58211476\n",
      "k 0.5378508\n",
      "k 0.7179198\n",
      "k 0.661599\n",
      "k 0.7474232\n",
      "k 0.7644475\n",
      "k 0.7516197\n",
      "k 0.8184168\n",
      "k 0.7691292\n",
      "k 0.8477307\n",
      "k 0.80619305\n",
      "k 0.8227203\n",
      "k 0.84692776\n",
      "k 0.82108057\n",
      "k 0.85904044\n",
      "k 0.84525293\n",
      "k 0.88045603\n",
      "k 0.8368992\n",
      "k 0.88372046\n",
      "k 0.8643837\n",
      "k 0.8634289\n",
      "k 0.85982627\n",
      "k 0.87308794\n",
      "k 0.8556378\n",
      "k 0.860137\n",
      "k 0.8756098\n",
      "k 0.87887305\n",
      "k 0.87697905\n",
      "k 0.8773066\n",
      "k 0.8857123\n",
      "k 0.89246446\n",
      "k 0.88799614\n",
      "k 0.8868098\n",
      "k 0.87473017\n",
      "k 0.870944\n",
      "k 0.8345226\n",
      "v 0.4343884\n",
      "none 0.988165\n",
      "x 0.43680453\n",
      "none 0.3990655\n",
      "g 0.4198625\n",
      "none 0.26998106\n",
      "none 0.988165\n",
      "a 0.65300715\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n",
      "none 0.988165\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.load_weights(SAVED_MODEL_PATH)\n",
    "\n",
    "predicted_sign = 'none'\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while cap.isOpened():\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    image = flip(image, 1)\n",
    "    \n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    image, results = mediapipe_detection(image, mp_hands)\n",
    "    draw_landmarks(image, results)\n",
    "    \n",
    "    keypoints = extract_absolute_keypoints(results)\n",
    "    data = normalize_landmarks2(keypoints)\n",
    "\n",
    "    prediction = model.predict(expand_dims(data, axis=0))[0]\n",
    "    predicted_class_index = argmax(prediction)\n",
    "\n",
    "    print(ALL_SIGNS[predicted_class_index], prediction[predicted_class_index])\n",
    "      \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    imshow(MODEL_NAME, image)\n",
    "    \n",
    "    key = cv2.waitKeyEx(1)\n",
    "    if key == 27:\n",
    "      break\n",
    "  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "data_path = join(DATA_DIR, 'e', 'e_11.64453258-ec50-11ed-bac7-1c872c4889a8.jpeg')\n",
    "\n",
    "# points = load(data_path)\n",
    "points = normalize_landmarks3(load(data_path))\n",
    "\n",
    "x = points[:, 0]\n",
    "y = points[:, 1]\n",
    "z = points[:, 2]\n",
    "\n",
    "# Set up the 3D scatter plot\n",
    "fig = plt.figure(figsize=(13, 13))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# https://developers.google.com/static/mediapipe/images/solutions/hand-landmarks.png\n",
    "pairs_to_connect = [\n",
    "  (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "  (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "  (0, 17), (17, 18), (18, 19), (19, 20),\n",
    "  (5, 9), (9, 13), (13, 17),\n",
    "  (9, 10), (10, 11), (11, 12),\n",
    "  (13, 14), (14, 15), (15, 16),\n",
    "]\n",
    "\n",
    "for pair in pairs_to_connect:\n",
    "  idx1, idx2 = pair\n",
    "  ax.plot(x[[idx1, idx2]], y[[idx1, idx2]], z[[idx1, idx2]], linestyle='-', linewidth=1)\n",
    "\n",
    "ax.scatter(x, y, z, s=100)\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "# Set the elevation and azimuth angles (in degrees)\n",
    "elevation_angle = -106\n",
    "azimuth_angle = -75\n",
    "ax.view_init(elevation_angle, azimuth_angle)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_difference(set1: np.ndarray, set2: np.ndarray) -> np.ndarray:\n",
    "  if set1.shape != set2.shape:\n",
    "    raise ValueError(\"Input sets should have the same shape.\")\n",
    "    \n",
    "  # Calculate the absolute differences between corresponding points\n",
    "  absolute_differences = np.abs(set1 - set2)\n",
    "  \n",
    "  # Calculate the sum of absolute differences for each axis\n",
    "  sum_differences = np.sum(absolute_differences, axis=0)\n",
    "  \n",
    "  # Calculate the sum of the values in set1 for each axis\n",
    "  sum_set1 = np.sum(set1, axis=0)\n",
    "  \n",
    "  # Calculate the percentage difference for each axis\n",
    "  percentage_difference = 100 * sum_differences / sum_set1\n",
    "  \n",
    "  return percentage_difference\n",
    "\n",
    "\n",
    "data_path = join(DATA_DIR, 'a', 'test.npy')\n",
    "\n",
    "dataRaw = load(data_path)\n",
    "datan1 = normalize_landmarks2(dataRaw)\n",
    "datan2 = normalize_landmarks3(dataRaw)\n",
    "\n",
    "\n",
    "print(average_difference(datan1, datan2))\n",
    "\n",
    "# headers = [\"x\", \"y\", \"z\"]\n",
    "# print(tabulate(datan2, headers=headers, tablefmt=\"pretty\", floatfmt=\".4f\", stralign='right', numalign='decimal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.14433208  0.9183103   0.36861151]\n",
      " [-0.33181997  0.67648386  0.23890351]\n",
      " [-0.43975745  0.53929761  0.06262659]\n",
      " [-0.42785707  0.37655977 -0.24102919]\n",
      " [-0.29140596  0.27988795 -0.41155994]\n",
      " [-0.22479712  0.03097031  0.12451288]\n",
      " [-0.14223019 -0.2081414   0.08017181]\n",
      " [-0.09050597 -0.34806946 -0.00781268]\n",
      " [-0.01962356 -0.49062893 -0.22221041]\n",
      " [-0.04782609  0.06815366  0.11410457]\n",
      " [ 0.06155949 -0.23770332  0.12188178]\n",
      " [ 0.13023567 -0.43832951  0.03395917]\n",
      " [ 0.18819581 -0.58504806 -0.15756596]\n",
      " [ 0.08668338  0.16375707  0.10557144]\n",
      " [ 0.16996133 -0.12584442  0.08326637]\n",
      " [ 0.21687545 -0.31251564 -0.03578173]\n",
      " [ 0.25594021 -0.46627984 -0.17202787]\n",
      " [ 0.18612346  0.33294978  0.06688976]\n",
      " [ 0.25827348  0.09061048  0.04748419]\n",
      " [ 0.29269642 -0.07272255 -0.04741743]\n",
      " [ 0.31361076 -0.19169767 -0.15257837]]\n"
     ]
    }
   ],
   "source": [
    "data_path = join(DATA_DIR, 'b', 'b_6.2e98b0e3-ec50-11ed-bcf0-1c872c4889a8.npy')\n",
    "my_data = load(data_path)\n",
    "print(my_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Export**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "model = get_model()\n",
    "model.load_weights(SAVED_MODEL_PATH)\n",
    "tfjs.converters.save_keras_model(model, EXPORTED_MODEL_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
