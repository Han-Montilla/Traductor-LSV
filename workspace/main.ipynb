{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS imports\n",
    "from os.path import join, realpath, dirname, exists, abspath, isfile, isdir\n",
    "from os import mkdir, name as os_name, getcwd, environ, pathsep, rename, listdir\n",
    "\n",
    "# Mediapipe imports\n",
    "from mediapipe.python.solutions import drawing_utils as du \n",
    "from mediapipe.python.solutions import hands\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "# Tensorflow & Keras imports\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.utils.all_utils import to_categorical\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras import optimizers\n",
    "\n",
    "# Data handeling/visualization imports\n",
    "import cv2\n",
    "from cv2 import imread, imshow, imwrite, flip, cvtColor, COLOR_BGR2RGB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "# General data handeling imports\n",
    "from numpy import array, zeros, concatenate, save, load, argmax, expand_dims, append\n",
    "import numpy as np\n",
    "from uuid import uuid1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options & Settings 💾\n",
    "MODEL_VERSION = 0.1\n",
    "MODEL_ARCH_ID = 'conv1'\n",
    "MODEL_DATA_SET = 'local1'\n",
    "\n",
    "MODEL_NAME = f'v{MODEL_VERSION}_arch-{MODEL_ARCH_ID}_data-{MODEL_DATA_SET}'\n",
    "\n",
    "MP_MODEL_COMPLEXITY = 0\n",
    "MP_DETECTION_CONFIDENCE = 0.75\n",
    "MP_TRACKING_CONFIDENCE = 0.75\n",
    "MP_NUM_HANDS = 1\n",
    "\n",
    "SIGNS = [\n",
    "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "\n",
    "ALL_SIGNS = SIGNS.copy()\n",
    "ALL_SIGNS.insert(0, 'none')\n",
    "CLASS_COUNT = len(ALL_SIGNS)\n",
    "\n",
    "# Constants 🚧\n",
    "HAND_LANDMARK_COUNT = 21 # https://mediapipe.dev/images/mobile/hand_landmarks.png\n",
    "HAND_POINT_COUNT = 3 # (x, y, z)\n",
    "HAND_LANDMARK_POINTS = HAND_LANDMARK_COUNT * HAND_POINT_COUNT # (x, y, z)\n",
    "\n",
    "# Paths 📁\n",
    "ROOT_DIR = getcwd()\n",
    "MODELS_DIR = join(ROOT_DIR, 'models')\n",
    "MODEL_DIR = join(MODELS_DIR, MODEL_NAME)\n",
    "LOG_DIR = join(MODEL_DIR, 'logs')\n",
    "SAVED_MODEL_PATH = join(MODEL_DIR, 'signs.h5')\n",
    "EXPORTED_MODEL_DIR = join(MODEL_DIR)\n",
    "DATA_DIR = join(ROOT_DIR, 'data')\n",
    "IMAGES_DIR = join(ROOT_DIR, 'images')\n",
    "TEST_IMAGES_DIR = join(IMAGES_DIR, 'test')\n",
    "COLLECTED_IMAGES_DIR = join(IMAGES_DIR, 'collected')\n",
    "PREPROCESSED_IMAGES_DIR = join(IMAGES_DIR, 'preprocessed')\n",
    "PROCCESSED_IMAGES_DIR = join(IMAGES_DIR, 'processed')\n",
    "REJECTED_IMAGES_DIR = join(IMAGES_DIR, 'rejected')\n",
    "\n",
    "# Global Utils 📐\n",
    "def create_dir(dir_path: str, notify: bool = True):\n",
    "  if not exists(dir_path):\n",
    "    mkdir(dir_path)\n",
    "  else:\n",
    "    if notify: print(f'{dir_path} already exists!')\n",
    "\n",
    "def dir_exists(dir_path: str) -> bool:\n",
    "  return exists(dir_path) and isdir(dir_path)\n",
    "\n",
    "EMPTY_KEYPOINTS = zeros((HAND_LANDMARK_COUNT, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mediapipe Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Tuple\n",
    "from numpy import ndarray\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "  if not results.multi_hand_landmarks: return image\n",
    "  \n",
    "  hand_landmarks = results.multi_hand_landmarks\n",
    "  \n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(119, 252, 3), thickness=1, circle_radius=2), \n",
    "      du.DrawingSpec(color=(119, 252, 3), thickness=1, circle_radius=2)\n",
    "    )\n",
    "    \n",
    "def draw_img_landmarks(image, hand_landmarks):\n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(224,0,0), thickness=32, circle_radius=5), # points\n",
    "      du.DrawingSpec(color=(0,0,224), thickness=32, circle_radius=5) # edges\n",
    "    )\n",
    "\n",
    "def mediapipe_detection(image, hands: hands.Hands):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # COLOR CONVERSION BGR 2 RGB\n",
    "  image.flags.writeable = False                   # Image is no longer writeable\n",
    "  results = hands.process(image)                  # Make prediction\n",
    "  image.flags.writeable = True                    # Image is now writeable\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # COLOR COVERSION RGB 2 BGR\n",
    "  return image, results\n",
    "\n",
    "def get_handedness(results: NamedTuple) -> str:\n",
    "  return MessageToDict(results.multi_handedness[0])['classification'][0]['label']\n",
    "\n",
    "def is_right_hand(results: NamedTuple) -> bool:\n",
    "  return get_handedness(results) == 'Right'\n",
    "\n",
    "def process_image(img_path: str):\n",
    "  if not exists(img_path): raise Exception('invalid img_path')\n",
    "\n",
    "  with hands.Hands(\n",
    "    model_complexity=MP_MODEL_COMPLEXITY,\n",
    "    min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "    min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "    max_num_hands=1\n",
    "  ) as mp_hands:\n",
    "    image = flip(imread(img_path), 1)\n",
    "    image, results = mediapipe_detection(image, mp_hands)\n",
    "\n",
    "def extract_absolute_keypoints(results: NamedTuple) -> ndarray:\n",
    "  \"\"\"\n",
    "  Recives raw mediapipe results tuple and produces a NDArray with the absolute positions of each landmark detected (if there are any)\n",
    "  \n",
    "  @param `results` mediapipe's hand solution output\n",
    "  @returns ndarray with shape of (21, 3) = (amount of landmarks in a single hand, 3 dimensional position (x, y,z))\n",
    "  \"\"\"\n",
    "  if not results.multi_hand_landmarks:\n",
    "    return EMPTY_KEYPOINTS\n",
    "  landmarks = MessageToDict(results.multi_hand_world_landmarks[0])['landmark']\n",
    "  res = []\n",
    "  for landmark in landmarks:\n",
    "    res.append([\n",
    "      landmark['x'],\n",
    "      landmark['y'],\n",
    "      landmark['z'],\n",
    "    ])\n",
    "  return array(res)\n",
    "\n",
    "# TODO!\n",
    "def normalize_landmarks(arr: ndarray):\n",
    "  \"\"\"\n",
    "  Transforms array of absolute positions into an array of normalized position within range [-1, 1]\n",
    "\n",
    "  @param `landmarks` an array of landmark positions of shape (21, 3)\n",
    "  @returns transformned NDArray of floats with shape (21, 3)\n",
    "  \"\"\"\n",
    "  # Compute the minimum and maximum values separately for each dimension\n",
    "  min_x, max_x = np.min(arr[:, 0]), np.max(arr[:, 0])\n",
    "  min_y, max_y = np.min(arr[:, 1]), np.max(arr[:, 1])\n",
    "  min_z, max_z = np.min(arr[:, 2]), np.max(arr[:, 2])\n",
    "\n",
    "  # Normalize each dimension separately\n",
    "  arr[:, 0] = (arr[:, 0] - min_x) / (max_x - min_x)\n",
    "  arr[:, 1] = (arr[:, 1] - min_y) / (max_y - min_y)\n",
    "  arr[:, 2] = (arr[:, 2] - min_z) / (max_z - min_z)\n",
    "\n",
    "  return arr\n",
    "\n",
    "def print_landmark_values(data: ndarray):\n",
    "  print(tabulate(data, headers=[\"x\", \"y\", \"z\"], floatfmt=\".2f\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Live Image Capture w/ Mediapipe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "selected_sign_index = 0\n",
    "selected_sign = SIGNS[selected_sign_index]\n",
    "selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def select_next_sign():\n",
    "  global selected_sign, selected_sign_index, selected_sign_amount\n",
    "  if selected_sign_index < len(SIGNS) - 1:\n",
    "    selected_sign_index += 1\n",
    "    selected_sign = SIGNS[selected_sign_index]\n",
    "    selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def select_prev_sign():\n",
    "  global selected_sign, selected_sign_index, selected_sign_amount\n",
    "  if selected_sign_index > 0:\n",
    "    selected_sign_index -= 1\n",
    "    selected_sign = SIGNS[selected_sign_index]\n",
    "    selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def capture(results):\n",
    "  global selected_sign, selected_sign_amount\n",
    "  if not results.multi_hand_landmarks:\n",
    "    print('no hand available')\n",
    "    return\n",
    "  if not is_right_hand(results):\n",
    "    print('must use right hand')\n",
    "    return\n",
    "\n",
    "  keypoints = extract_absolute_keypoints(results)\n",
    "  data_path = join(DATA_DIR, selected_sign, f'{selected_sign}_{selected_sign_amount}.{uuid1()}')\n",
    "  save(data_path, keypoints)\n",
    "  print(f'saved at {data_path}')\n",
    "  selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while cap.isOpened():\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    image = flip(image, 1)\n",
    "    \n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    image, results = mediapipe_detection(image, mp_hands)\n",
    "    draw_landmarks(image, results)\n",
    "    image = cv2.putText(\n",
    "      image, f'{selected_sign} | {selected_sign_amount}',\n",
    "      (32, 32), cv2.FONT_HERSHEY_SIMPLEX, 1, (119, 252, 3), 2, cv2.LINE_AA\n",
    "    )\n",
    "      \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    imshow('signs', image)\n",
    "    \n",
    "    key = cv2.waitKeyEx(1)\n",
    "\n",
    "    if key == 32: # space bar\n",
    "      capture(results)\n",
    "\n",
    "    if key == 2555904: # right key\n",
    "      select_next_sign()\n",
    "\n",
    "    if key == 2424832: # left key\n",
    "      select_prev_sign()\n",
    "      \n",
    "    if key == 27:\n",
    "      break\n",
    "  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*generate folders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for s in SIGNS:\n",
    "  create_dir(join(DATA_DIR, s), False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1():\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(21, 3)))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Loading & Partitioning data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_count():\n",
    "  data_amounts = []\n",
    "  for collection_dir in listdir(DATA_DIR):\n",
    "    if collection_dir == '.gitkeep': continue\n",
    "    data_amounts.append(len(listdir(join(DATA_DIR, collection_dir))))\n",
    "      \n",
    "  return min(data_amounts)\n",
    "\n",
    "COLLECTION_COUNT = get_collection_count()\n",
    "print(f'currently using {COLLECTION_COUNT} data points')\n",
    "\n",
    "# Load Training Data\n",
    "label_map = { label: num for num, label in enumerate(ALL_SIGNS) }\n",
    "sequences, labels = zeros((COLLECTION_COUNT, 21, 3)), [ 0 for i in range(COLLECTION_COUNT) ]\n",
    "\n",
    "for sign in SIGNS:\n",
    "  sign_data_dir = join(DATA_DIR, sign)\n",
    "  for data_file_name in listdir(sign_data_dir)[:COLLECTION_COUNT]:\n",
    "    data_path = join(sign_data_dir, data_file_name)\n",
    "    res = load(data_path)\n",
    "    sequence = append(sequences, res)\n",
    "    labels.append(label_map[sign])\n",
    "\n",
    "x = array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "# Testing!\n",
    "s_expected = (CLASS_COUNT * COLLECTION_COUNT, HAND_LANDMARK_COUNT, HAND_POINT_COUNT)\n",
    "s_result = x.shape\n",
    "l_expected = (CLASS_COUNT * COLLECTION_COUNT, CLASS_COUNT)\n",
    "l_result = y.shape\n",
    "if s_result != s_expected:\n",
    "  raise Exception(f'WARNING: expected sequence shape `{s_expected}` != from gotten `{s_result}`')\n",
    "if l_result != l_expected:\n",
    "  raise Exception(f'WARNING: expected labels shape `{l_expected}` != from gotten `{l_result}`')\n",
    "\n",
    "# partitioning train, test, validation data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1 * 1.0)            # 10% test data (1.0 * 0.20 = 0.1 => 10%)\n",
    "x_train, x_val,  y_train, y_val  = train_test_split(x_train, y_train, test_size=0.1/0.9)  # 10% val  data (0.9 * 0.11 = 0.1 => 10%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image_path = join(TEST_IMAGES_DIR, '1.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
