{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OS imports\n",
    "from os.path import join, realpath, dirname, exists, abspath, isfile, isdir, basename\n",
    "from os import mkdir, name as os_name, getcwd, environ, pathsep, rename, listdir, remove\n",
    "\n",
    "# Mediapipe imports\n",
    "from mediapipe.python.solutions import drawing_utils as du \n",
    "from mediapipe.python.solutions import hands\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "# Tensorflow & Keras imports\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "from tensorflow.python.keras.utils.all_utils import to_categorical\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras import optimizers\n",
    "from tensorflow.python.keras import losses, metrics\n",
    "\n",
    "# Data handeling/visualization imports\n",
    "import cv2\n",
    "from cv2 import imread, imshow, imwrite, flip, cvtColor, COLOR_BGR2RGB\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tabulate import tabulate\n",
    "\n",
    "# General data handeling imports\n",
    "from numpy import array, zeros, concatenate, save, load, argmax, expand_dims, append\n",
    "import numpy as np\n",
    "from uuid import uuid1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options & Settings 💾\n",
    "MODEL_VERSION = '1'\n",
    "MODEL_ARCH_ID = 'conv1'\n",
    "MODEL_DATA_SET = 'aTOz'\n",
    "MODEL_NORMFUNC = 'n2'\n",
    "MODEL_EXTRACTION_TYPE = 'absolute'\n",
    "MODEL_DATA_PIPELINE = f'{MODEL_EXTRACTION_TYPE}{MODEL_NORMFUNC}'\n",
    "\n",
    "MODEL_NAME = f'v{MODEL_VERSION}_{MODEL_ARCH_ID}_data-{MODEL_DATA_SET}_pipeline-{MODEL_DATA_PIPELINE}'\n",
    "\n",
    "MP_MODEL_COMPLEXITY = 0\n",
    "MP_DETECTION_CONFIDENCE = 0.75\n",
    "MP_TRACKING_CONFIDENCE = 0.75\n",
    "MP_NUM_HANDS = 1\n",
    "\n",
    "SIGNS = [\n",
    "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "\n",
    "ALL_SIGNS = SIGNS.copy()\n",
    "ALL_SIGNS.insert(0, 'none')\n",
    "CLASS_COUNT = len(ALL_SIGNS)\n",
    "\n",
    "# Constants 🚧\n",
    "HAND_LANDMARK_COUNT = 21 # https://mediapipe.dev/images/mobile/hand_landmarks.png\n",
    "HAND_POINT_COUNT = 3 # (x, y, z)\n",
    "HAND_LANDMARK_POINTS = HAND_LANDMARK_COUNT * HAND_POINT_COUNT # (x, y, z)\n",
    "\n",
    "# Paths 📁\n",
    "ROOT_DIR = getcwd()\n",
    "MODELS_DIR = join(ROOT_DIR, 'models')\n",
    "MODEL_DIR = join(MODELS_DIR, MODEL_NAME)\n",
    "MP_DIR = join(ROOT_DIR, 'mediapipe')\n",
    "MP_MODEL_PATH = join(MP_DIR, 'models', 'hand_landmarker.task')\n",
    "LOG_DIR = join(MODEL_DIR, 'logs')\n",
    "SAVED_MODEL_PATH = join(MODEL_DIR, 'signs.h5')\n",
    "EXPORTED_MODEL_DIR = join(MODEL_DIR)\n",
    "DATA_DIR = join(ROOT_DIR, 'data')\n",
    "IMAGES_DIR = join(ROOT_DIR, 'images')\n",
    "COLLECTED_IMAGES_DIR = join(IMAGES_DIR, 'collected')\n",
    "ACCEPTED_IMAGES_DIR = join(IMAGES_DIR, 'accepted')\n",
    "REJECTED_IMAGES_DIR = join(IMAGES_DIR, 'rejected')\n",
    "TEST_IMAGES_DIR = join(IMAGES_DIR, 'test')\n",
    "\n",
    "# Global Utils 📐\n",
    "def create_dir(dir_path: str, notify: bool = True):\n",
    "  if not exists(dir_path):\n",
    "    mkdir(dir_path)\n",
    "  else:\n",
    "    if notify: print(f'{dir_path} already exists!')\n",
    "\n",
    "def dir_exists(dir_path: str) -> bool:\n",
    "  return exists(dir_path) and isdir(dir_path)\n",
    "\n",
    "EMPTY_KEYPOINTS = zeros((HAND_LANDMARK_COUNT, 3))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mediapipe Utilities**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import NamedTuple, Tuple\n",
    "from numpy import ndarray\n",
    "\n",
    "def draw_landmarks(image, results):\n",
    "  if not results.multi_hand_landmarks: return image\n",
    "  \n",
    "  hand_landmarks = results.multi_hand_landmarks\n",
    "  \n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(119, 252, 3), thickness=1, circle_radius=2), \n",
    "      du.DrawingSpec(color=(119, 252, 3), thickness=1, circle_radius=2)\n",
    "    )\n",
    "    \n",
    "def draw_img_landmarks(image, hand_landmarks):\n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(224,0,0), thickness=32, circle_radius=5), # points\n",
    "      du.DrawingSpec(color=(0,0,224), thickness=32, circle_radius=5) # edges\n",
    "    )\n",
    "\n",
    "def mediapipe_detection(image, hands: hands.Hands):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # COLOR CONVERSION BGR 2 RGB\n",
    "  image.flags.writeable = False                   # Image is no longer writeable\n",
    "  results = hands.process(image)                  # Make prediction\n",
    "  image.flags.writeable = True                    # Image is now writeable\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # COLOR COVERSION RGB 2 BGR\n",
    "  return image, results\n",
    "\n",
    "def get_handedness(results: NamedTuple) -> str:\n",
    "  return MessageToDict(results.multi_handedness[0])['classification'][0]['label']\n",
    "\n",
    "def is_right_hand(results: NamedTuple) -> bool:\n",
    "  return get_handedness(results) == 'Right'\n",
    "\n",
    "def extract_keypoints(results: NamedTuple) -> ndarray:\n",
    "  if not results.multi_hand_landmarks:\n",
    "    return EMPTY_KEYPOINTS\n",
    "  landmarks = MessageToDict(results.multi_hand_landmarks[0])['landmark']\n",
    "\n",
    "  res = []\n",
    "  for landmark in landmarks:\n",
    "    x = landmark['x']\n",
    "    y = landmark['y']\n",
    "    z = landmark['z']\n",
    "    res.append([ x, y, z ])\n",
    "\n",
    "  return array(res)\n",
    "\n",
    "def extract_absolute_keypoints(results: NamedTuple) -> ndarray:\n",
    "  if not results.multi_hand_landmarks:\n",
    "    return EMPTY_KEYPOINTS\n",
    "  landmarks = MessageToDict(results.multi_hand_world_landmarks[0])['landmark']\n",
    "  # landmarks = MessageToDict(results.multi_hand_landmarks[0])['landmark']\n",
    "\n",
    "  res = []\n",
    "  for landmark in landmarks:\n",
    "    x = landmark['x']\n",
    "    y = landmark['y']\n",
    "    z = landmark['z']\n",
    "    res.append([ x, y, z ])\n",
    "\n",
    "  return array(res)\n",
    "\n",
    "\n",
    "def normalize_landmarks(arr: ndarray):\n",
    "\n",
    "  min_x, max_x = np.min(arr[:, 0]), np.max(arr[:, 0])\n",
    "  min_y, max_y = np.min(arr[:, 1]), np.max(arr[:, 1])\n",
    "  min_z, max_z = np.min(arr[:, 2]), np.max(arr[:, 2])\n",
    "\n",
    "  arr[:, 0] = (arr[:, 0] - min_x) / (max_x - min_x)\n",
    "  arr[:, 1] = (arr[:, 1] - min_y) / (max_y - min_y)\n",
    "  arr[:, 2] = (arr[:, 2] - min_z) / (max_z - min_z)\n",
    "\n",
    "  return arr\n",
    "\n",
    "def normalize_landmarks2(landmarks: ndarray):\n",
    "\n",
    "  centroid = np.mean(landmarks, axis=0)\n",
    "\n",
    "  translated_landmarks = landmarks - centroid\n",
    "\n",
    "  scale_factor = np.max(np.linalg.norm(translated_landmarks, axis=1))\n",
    "\n",
    "  if scale_factor == 0:\n",
    "    return translated_landmarks\n",
    "\n",
    "  normalized_landmarks = translated_landmarks / scale_factor\n",
    "\n",
    "  return normalized_landmarks\n",
    "\n",
    "def normalize_landmarks3(landmarks: ndarray) -> ndarray:\n",
    "\n",
    "  min_values = np.min(landmarks, axis=0)\n",
    "  max_values = np.max(landmarks, axis=0)\n",
    "\n",
    "  bounding_box_dimensions = max_values - min_values\n",
    "\n",
    "  scaling_factor = np.max(bounding_box_dimensions)\n",
    "\n",
    "  translated_landmarks = landmarks - min_values\n",
    "\n",
    "  scaled_landmarks = translated_landmarks / scaling_factor\n",
    "\n",
    "  min_values_scaled = np.min(scaled_landmarks, axis=0)\n",
    "  normalized_landmarks = scaled_landmarks - min_values_scaled\n",
    "\n",
    "  return normalized_landmarks\n",
    "\n",
    "def print_landmark_values(data: ndarray):\n",
    "  print(tabulate(data, headers=[\"x\", \"y\", \"z\"], floatfmt=\".4f\", stralign='right', numalign='decimal'))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Collection & Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Camera Check\n",
    "\n",
    "camera_index = 0\n",
    "while True:\n",
    "    cap = cv2.VideoCapture(camera_index, cv2.CAP_DSHOW)\n",
    "    if not cap.read()[0]:\n",
    "        break\n",
    "    else:\n",
    "        print(f\"Camera index: {camera_index} - OK\")\n",
    "    cap.release()\n",
    "    camera_index += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Live Image collection\n",
    "\n",
    "test_mode = True\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "selected_sign_index = 0\n",
    "selected_sign = SIGNS[selected_sign_index]\n",
    "selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def select_next_sign():\n",
    "  global selected_sign, selected_sign_index, selected_sign_amount\n",
    "  if selected_sign_index < len(SIGNS) - 1:\n",
    "    selected_sign_index += 1\n",
    "    selected_sign = SIGNS[selected_sign_index]\n",
    "    selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def select_prev_sign():\n",
    "  global selected_sign, selected_sign_index, selected_sign_amount\n",
    "  if selected_sign_index > 0:\n",
    "    selected_sign_index -= 1\n",
    "    selected_sign = SIGNS[selected_sign_index]\n",
    "    selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def capture(results, image):\n",
    "  global selected_sign, selected_sign_amount\n",
    "  if not results.multi_hand_landmarks:\n",
    "    print('no hand available')\n",
    "    return\n",
    "  if not is_right_hand(results):\n",
    "    print('must use right hand')\n",
    "    return\n",
    "\n",
    "  keypoints = extract_absolute_keypoints(results) # absolute\n",
    "  data = normalize_landmarks2(keypoints) # n2\n",
    "\n",
    "  generated_name = f'{selected_sign}_{selected_sign_amount}.{uuid1()}'\n",
    "  data_path = join(DATA_DIR, selected_sign, generated_name)\n",
    "  img_path = join(TEST_IMAGES_DIR, generated_name) if test_mode else join(COLLECTED_IMAGES_DIR, selected_sign, generated_name)\n",
    "  img_path += '.jpeg'\n",
    "  if not test_mode: save(data_path, data)\n",
    "  imwrite(img_path, image)\n",
    "  print(f'saved {generated_name}')\n",
    "  selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while cap.isOpened():\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    raw_image = image\n",
    "    image = flip(image, 1)\n",
    "    \n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    image, results = mediapipe_detection(image, mp_hands)\n",
    "    draw_landmarks(image, results)\n",
    "    show_img = image\n",
    "    if not test_mode:\n",
    "      show_img = cv2.putText(\n",
    "        show_img, f'{selected_sign} | {selected_sign_amount}',\n",
    "        (32, 32), cv2.FONT_HERSHEY_SIMPLEX, 1, (119, 252, 3), 2, cv2.LINE_AA\n",
    "      )\n",
    "      \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    imshow('signs', show_img)\n",
    "    \n",
    "    key = cv2.waitKeyEx(1)\n",
    "\n",
    "    if key == 32: # space bar\n",
    "      capture(results, show_img if test_mode else raw_image)\n",
    "\n",
    "    if key == 2555904: # right key\n",
    "      select_next_sign()\n",
    "\n",
    "    if key == 2424832: # left key\n",
    "      select_prev_sign()\n",
    "      \n",
    "    if key == 27:\n",
    "      break\n",
    "  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Image selection TODO!\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "image_extensions = ['.jpg', '.jpeg', '.png', '.bmp', '.tiff']\n",
    "\n",
    "def get_image_paths(directory):\n",
    "  image_paths = {}\n",
    "\n",
    "  for subdir in listdir(directory):\n",
    "    subdir_path = join(directory, subdir)\n",
    "    if isdir(subdir_path):\n",
    "      images = []\n",
    "      for filename in listdir(subdir_path):\n",
    "        if any(filename.endswith(ext) for ext in image_extensions):\n",
    "          images.append(join(subdir_path, filename))\n",
    "      image_paths[subdir] = images\n",
    "\n",
    "  return image_paths\n",
    "\n",
    "show_landmarks = True\n",
    "selected_sign = SIGNS[0]\n",
    "selected_img_index = 0\n",
    "\n",
    "images_paths = get_image_paths(COLLECTED_IMAGES_DIR)\n",
    "\n",
    "def create_black_image(width, height):\n",
    "  return zeros((height, width, 3), dtype=np.uint8)\n",
    "\n",
    "def move_files(img_id, img):\n",
    "  pass\n",
    "\n",
    "def accept(img):\n",
    "  pass\n",
    "\n",
    "\n",
    "def reject(img):\n",
    "  pass\n",
    "\n",
    "\n",
    "def move_right():\n",
    "  pass\n",
    "\n",
    "def move_left():\n",
    "  pass\n",
    "\n",
    "def move_up():\n",
    "  pass\n",
    "\n",
    "def move_down():\n",
    "  pass\n",
    "\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while True:\n",
    "\n",
    "    key = cv2.waitKey(0)\n",
    "\n",
    "    can_do_stuff = len(images_paths[selected_sign]) > 0\n",
    "\n",
    "    image = create_black_image(640, 480)\n",
    "\n",
    "    if can_do_stuff:\n",
    "      img_path = images_paths[selected_sign][selected_img_index]\n",
    "      image = cv2.imread(img_path)\n",
    "    else:\n",
    "      # add some text to blck screen\n",
    "      pass\n",
    "\n",
    "    imshow('signs', image)\n",
    "\n",
    "    if key == 101 and can_do_stuff: # e \"accept\"\n",
    "      accept(image)\n",
    "\n",
    "    if key == 113 and can_do_stuff: # q \"reject\"\n",
    "      reject(image)\n",
    "\n",
    "    if key == 2555904 and can_do_stuff: # right key\n",
    "      move_right()\n",
    "\n",
    "    if key == 2424832 and can_do_stuff: # left key\n",
    "      move_left()\n",
    "\n",
    "    if key == 2490368: # up arrow key\n",
    "      move_up()\n",
    "\n",
    "    if key == 2621440: # down arrow key\n",
    "      move_down()\n",
    "      \n",
    "    if key == 27:\n",
    "      break\n",
    "\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*generate empty folders*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directories = [\n",
    "  COLLECTED_IMAGES_DIR,\n",
    "  ACCEPTED_IMAGES_DIR,\n",
    "  REJECTED_IMAGES_DIR,\n",
    "  DATA_DIR\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "  for s in SIGNS:\n",
    "    create_dir(join(directory, s), False)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv1():\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(21, 3)))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
    "\n",
    "  return model\n",
    "\n",
    "def conv2():\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(21, 3)))\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "  model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
    "\n",
    "  return model\n",
    "\n",
    "def conv3():\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(21, 3)))\n",
    "  model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Dropout(0.5))\n",
    "\n",
    "  model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "  model.add(Conv1D(filters=128, kernel_size=3, activation='relu'))\n",
    "  model.add(MaxPooling1D(pool_size=2))\n",
    "  model.add(Dropout(0.5))\n",
    "\n",
    "  model.add(Flatten())\n",
    "  model.add(Dense(256, activation='relu'))\n",
    "  model.add(Dropout(0.5))\n",
    "  model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
    "\n",
    "  return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Training**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Model Selection*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model() -> Sequential:\n",
    "  return conv1()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Loading & Partitioning data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_count():\n",
    "  data_amounts = []\n",
    "  for sign in SIGNS:\n",
    "    data_directory = join(DATA_DIR, sign)\n",
    "    data_amounts.append(len(listdir(join(DATA_DIR, data_directory))))\n",
    "\n",
    "      \n",
    "  return min(data_amounts)\n",
    "\n",
    "COLLECTION_COUNT = get_collection_count()\n",
    "print(f'currently using {COLLECTION_COUNT} data points')\n",
    "\n",
    "# Load Training Data\n",
    "label_map = { label: num for num, label in enumerate(ALL_SIGNS) }\n",
    "sequences, labels = [zeros((COLLECTION_COUNT, 21, 3)), [ 0 for _ in range(COLLECTION_COUNT) ]]\n",
    "\n",
    "\n",
    "for sign in SIGNS:\n",
    "  sign_data_dir = join(DATA_DIR, sign)\n",
    "  for data_file_name in listdir(sign_data_dir)[:COLLECTION_COUNT]:\n",
    "    data_path = join(sign_data_dir, data_file_name)\n",
    "    res = load(data_path)\n",
    "    expanded_res = expand_dims(res, axis=0)\n",
    "    sequences = concatenate((sequences, expanded_res), axis=0)\n",
    "    labels.append(label_map[sign])\n",
    "\n",
    "x = array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "# Testing!\n",
    "x_shape_expected = (CLASS_COUNT * COLLECTION_COUNT, HAND_LANDMARK_COUNT, HAND_POINT_COUNT)\n",
    "x_shape_result = x.shape\n",
    "y_shape_expected = (CLASS_COUNT * COLLECTION_COUNT, CLASS_COUNT)\n",
    "y_shape_result = y.shape\n",
    "if x_shape_result != x_shape_expected:\n",
    "  raise Exception(f'ERROR: expected sequence shape `{x_shape_expected}` != from gotten `{x_shape_result}`')\n",
    "if y_shape_result != y_shape_expected:\n",
    "  raise Exception(f'ERROR: expected labels shape `{y_shape_expected}` != from gotten `{y_shape_result}`')\n",
    "\n",
    "# partitioning train, test, validation data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1 * 1.0)            # 10% test data (1.0 * 0.20 = 0.1 => 10%)\n",
    "x_train, x_val,  y_train, y_val  = train_test_split(x_train, y_train, test_size=0.1/0.9)  # 10% val  data (0.9 * 0.11 = 0.1 => 10%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStoppingByLoss(tf.keras.callbacks.Callback):\n",
    "  def __init__(self, target_loss):\n",
    "    super(EarlyStoppingByLoss, self).__init__()\n",
    "    self.target_loss = target_loss\n",
    "\n",
    "  def on_epoch_end(self, epoch, logs=None):\n",
    "    current_loss = logs.get('loss')\n",
    "    if current_loss is not None and current_loss <= self.target_loss:\n",
    "      print(f\"\\nReached target loss {self.target_loss}, stopping training.\")\n",
    "      self.model.stop_training = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(MODEL_DIR)\n",
    "mkdir(LOG_DIR)\n",
    "\n",
    "tb_callback = TensorBoard(log_dir=LOG_DIR)\n",
    "early_stopping_by_loss = EarlyStoppingByLoss(target_loss=0.1)\n",
    "terminal_log_dir = LOG_DIR.replace('\\\\', '/')\n",
    "\n",
    "model = get_model()\n",
    "\n",
    "model.compile(\n",
    "  optimizer=optimizers.adam_v2.Adam(learning_rate=0.000025),\n",
    "  loss=losses.CategoricalCrossentropy(),\n",
    "  metrics=[metrics.CategoricalCrossentropy(), metrics.CategoricalAccuracy()]\n",
    ")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(f'[RUN]: tensorboard --logdir={terminal_log_dir}')\n",
    "\n",
    "model.fit(\n",
    "  x_train, y_train, validation_data=(x_val, y_val),\n",
    "  epochs=10000, batch_size=10,\n",
    "  use_multiprocessing=True, workers=4,\n",
    "  callbacks=[tb_callback],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(SAVED_MODEL_PATH)\n",
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Testing**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL 💾\n",
    "model = get_model()\n",
    "model.load_weights(SAVED_MODEL_PATH)\n",
    "print(model.input_shape)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-TRAINING PREDICIONS ⚡\n",
    "res = model.predict(x_test)\n",
    "\n",
    "y_real = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(res)):\n",
    "  y_real.append(argmax(y_test[i]))\n",
    "  y_pred.append(argmax(res[i]))\n",
    "\n",
    "y_real = array(y_real)\n",
    "y_pred = array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX 😵\n",
    "cm = confusion_matrix(y_real, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=ALL_SIGNS)\n",
    "disp.plot(xticks_rotation=75)\n",
    "disp.figure_.set_size_inches(12.8, 7.2)\n",
    "disp.figure_.set_dpi(150)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Predict*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.load_weights(SAVED_MODEL_PATH)\n",
    "\n",
    "predicted_sign = 'none'\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while cap.isOpened():\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    image = flip(image, 1)\n",
    "    \n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    image, results = mediapipe_detection(image, mp_hands)\n",
    "    draw_landmarks(image, results)\n",
    "    \n",
    "    keypoints = extract_absolute_keypoints(results)\n",
    "    data = normalize_landmarks2(keypoints)\n",
    "\n",
    "    prediction = model.predict(expand_dims(data, axis=0))[0]\n",
    "    predicted_class_index = argmax(prediction)\n",
    "\n",
    "    print(ALL_SIGNS[predicted_class_index], prediction[predicted_class_index])\n",
    "      \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    imshow(MODEL_NAME, image)\n",
    "    \n",
    "    key = cv2.waitKeyEx(1)\n",
    "    if key == 27:\n",
    "      break\n",
    "  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "data_path = join(DATA_DIR, 'e', 'e_16.64def6df-ec50-11ed-afbf-1c872c4889a8.npy')\n",
    "\n",
    "# points = load(data_path)\n",
    "points = normalize_landmarks3(load(data_path))\n",
    "\n",
    "x = points[:, 0]\n",
    "y = points[:, 1]\n",
    "z = points[:, 2]\n",
    "\n",
    "# Set up the 3D scatter plot\n",
    "fig = plt.figure(figsize=(13, 13))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# https://developers.google.com/static/mediapipe/images/solutions/hand-landmarks.png\n",
    "pairs_to_connect = [\n",
    "  (0, 1), (1, 2), (2, 3), (3, 4),\n",
    "  (0, 5), (5, 6), (6, 7), (7, 8),\n",
    "  (0, 17), (17, 18), (18, 19), (19, 20),\n",
    "  (5, 9), (9, 13), (13, 17),\n",
    "  (9, 10), (10, 11), (11, 12),\n",
    "  (13, 14), (14, 15), (15, 16),\n",
    "]\n",
    "\n",
    "for pair in pairs_to_connect:\n",
    "  idx1, idx2 = pair\n",
    "  ax.plot(x[[idx1, idx2]], y[[idx1, idx2]], z[[idx1, idx2]], linestyle='-', linewidth=1)\n",
    "\n",
    "ax.scatter(x, y, z, s=100)\n",
    "\n",
    "ax.set_xlabel('X Label')\n",
    "ax.set_ylabel('Y Label')\n",
    "ax.set_zlabel('Z Label')\n",
    "\n",
    "# Set the elevation and azimuth angles (in degrees)\n",
    "elevation_angle = -106\n",
    "azimuth_angle = -75\n",
    "ax.view_init(elevation_angle, azimuth_angle)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_difference(set1: np.ndarray, set2: np.ndarray) -> np.ndarray:\n",
    "  if set1.shape != set2.shape:\n",
    "    raise ValueError(\"Input sets should have the same shape.\")\n",
    "    \n",
    "  # Calculate the absolute differences between corresponding points\n",
    "  absolute_differences = np.abs(set1 - set2)\n",
    "  \n",
    "  # Calculate the sum of absolute differences for each axis\n",
    "  sum_differences = np.sum(absolute_differences, axis=0)\n",
    "  \n",
    "  # Calculate the sum of the values in set1 for each axis\n",
    "  sum_set1 = np.sum(set1, axis=0)\n",
    "  \n",
    "  # Calculate the percentage difference for each axis\n",
    "  percentage_difference = 100 * sum_differences / sum_set1\n",
    "  \n",
    "  return percentage_difference\n",
    "\n",
    "\n",
    "data_path = join(DATA_DIR, 'a', 'test.npy')\n",
    "\n",
    "dataRaw = load(data_path)\n",
    "datan1 = normalize_landmarks2(dataRaw)\n",
    "datan2 = normalize_landmarks3(dataRaw)\n",
    "\n",
    "\n",
    "print(average_difference(datan1, datan2))\n",
    "\n",
    "# headers = [\"x\", \"y\", \"z\"]\n",
    "# print(tabulate(datan2, headers=headers, tablefmt=\"pretty\", floatfmt=\".4f\", stralign='right', numalign='decimal'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = join(DATA_DIR, 'b', 'b_6.2e98b0e3-ec50-11ed-bcf0-1c872c4889a8.npy')\n",
    "my_data = load(data_path)\n",
    "print(my_data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Export**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.load_weights(SAVED_MODEL_PATH)\n",
    "tfjs.converters.save_keras_model(model, EXPORTED_MODEL_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
