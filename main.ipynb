{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, realpath, dirname, exists, abspath, isfile, isdir\n",
    "from os import mkdir as mk, name as os_name, getcwd, environ, pathsep, rename, listdir\n",
    "from typing import Tuple\n",
    "\n",
    "from mediapipe.python.solutions import drawing_utils as du \n",
    "from mediapipe.python.solutions import hands\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils.all_utils import to_categorical;\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import LSTM, Dense\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "\n",
    "from numpy import array, zeros, concatenate, save, load\n",
    "from uuid import uuid1\n",
    "\n",
    "import cv2\n",
    "from cv2 import imread, imshow, imwrite, flip, cvtColor, COLOR_BGR2RGB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options ðŸ’¾\n",
    "MODEL_NAME = 'v1'\n",
    "\n",
    "MP_MODEL_COMPLEXITY = 0\n",
    "MP_DETECTION_CONFIDENCE = 0.5\n",
    "MP_TRACKING_CONFIDENCE = 0.5\n",
    "MP_NUM_HANDS = 1\n",
    "\n",
    "SIGNS = [\n",
    "  'a', 'b', 'c',\n",
    "]\n",
    "\n",
    "ALL_SIGNS = SIGNS.copy()\n",
    "ALL_SIGNS.insert(0, 'none')\n",
    "\n",
    "CLASS_COUNT = len(ALL_SIGNS)\n",
    "# COLLECTION_COUNT = 100 # Amount of data points per class\n",
    "SEQUENCE_LENGHT = 10 # Amount of data per collection\n",
    "\n",
    "# Paths ðŸ“\n",
    "ROOT_DIR = getcwd()\n",
    "MODELS_DIR = join(ROOT_DIR, 'models')\n",
    "MODEL_DIR = join(MODELS_DIR, MODEL_NAME)\n",
    "LOG_DIR = join(MODEL_DIR, 'logs')\n",
    "DATA_DIR = join(ROOT_DIR, 'data')\n",
    "IMAGES_DIR = join(ROOT_DIR, 'images')\n",
    "COLLECTED_IMAGES_DIR = join(IMAGES_DIR, 'collected')\n",
    "PREPROCESSED_IMAGES_DIR = join(IMAGES_DIR, 'preprocessed')\n",
    "PROCCESSED_IMAGES_DIR = join(IMAGES_DIR, 'processed')\n",
    "REJECTED_IMAGES_DIR = join(IMAGES_DIR, 'rejected')\n",
    "\n",
    "# Constants ðŸš§\n",
    "HAND_LANDMARK_COUNT = 21 # https://mediapipe.dev/images/mobile/hand_landmarks.png\n",
    "HAND_LANDMARK_POINTS = HAND_LANDMARK_COUNT * 3 # (x, y, z)\n",
    "\n",
    "# Util ðŸ“\n",
    "def mkdir(path: str):\n",
    "  if not exists(path):\n",
    "    mk(path)\n",
    "  else:\n",
    "    print(f'{path} already exists!')\n",
    "      \n",
    "def dir_exists(dir_path: str) -> bool:\n",
    "  return exists(dir_path) and isdir(dir_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mediapipe Util**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, hand_landmarks):\n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "      du.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "    )\n",
    "    \n",
    "def draw_img_landmarks(image, hand_landmarks):\n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(224,0,0), thickness=32, circle_radius=5), # points\n",
    "      du.DrawingSpec(color=(0,0,224), thickness=32, circle_radius=5) # edges\n",
    "    )\n",
    "\n",
    "def mediapipe_detection(image, hands: hands.Hands):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # COLOR CONVERSION BGR 2 RGB\n",
    "  image.flags.writeable = False                   # Image is no longer writeable\n",
    "  results = hands.process(image)                  # Make prediction\n",
    "  image.flags.writeable = True                    # Image is now writeable\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # COLOR COVERSION RGB 2 BGR\n",
    "  return image, results\n",
    "\n",
    "def extract_keypoints_rh(results):\n",
    "  landmarks = MessageToDict(results.multi_hand_landmarks[0])['landmark']\n",
    "  res = []\n",
    "  \n",
    "  for lk in landmarks:\n",
    "    res.append(lk['x'])\n",
    "    res.append(lk['y'])\n",
    "    res.append(lk['z'])\n",
    "    \n",
    "  return array(res)\n",
    "\n",
    "def get_handedness(results):\n",
    "  return MessageToDict(results.multi_handedness[0])['classification'][0]['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model Structures*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_0(input_shape: Tuple[int, int]) -> Sequential:\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=input_shape))\n",
    "  model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "  model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
    "  return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Capture w/Mediapipe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while cap.isOpened():\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    image = flip(image, 1)\n",
    "    \n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    image, results = mediapipe_detection(image, mp_hands)\n",
    "    if results.multi_hand_landmarks:\n",
    "      draw_landmarks(image, results.multi_hand_landmarks)\n",
    "      \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    imshow('MediaPipe Hands', image)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "      break\n",
    "  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Renaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SIGN = 'c'\n",
    "SOURCE_SIGN_DIR = join(COLLECTED_IMAGES_DIR, _SIGN)\n",
    "DESTIN_SIGN_DIR = join(PREPROCESSED_IMAGES_DIR, _SIGN)\n",
    "mkdir(DESTIN_SIGN_DIR)\n",
    "\n",
    "if not exists(SOURCE_SIGN_DIR): raise Exception(f'make sure {SOURCE_SIGN_DIR} exists!')\n",
    "\n",
    "for img_name in listdir(SOURCE_SIGN_DIR):\n",
    "  src_img_path = join(SOURCE_SIGN_DIR, img_name)\n",
    "  if isfile(src_img_path):\n",
    "    dest_img_path = join(DESTIN_SIGN_DIR, f'{_SIGN}.{uuid1()}.jpg')\n",
    "    rename(src_img_path, dest_img_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Data Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject(sign: str, img_name: str):\n",
    "  rename(\n",
    "    join(PREPROCESSED_IMAGES_DIR, sign, img_name),\n",
    "    join(REJECTED_IMAGES_DIR, img_name)\n",
    "  )\n",
    "  \n",
    "def accept(sign: str, img_name: str):\n",
    "  rename(\n",
    "    join(PREPROCESSED_IMAGES_DIR, sign, img_name),\n",
    "    join(PROCCESSED_IMAGES_DIR, img_name)\n",
    "  )\n",
    "\n",
    "for sign in SIGNS:\n",
    "  SIGN_DIR = join(PREPROCESSED_IMAGES_DIR, sign)\n",
    "  DATA_SIGN_DIR = join(DATA_DIR, sign)\n",
    "  if not exists(DATA_SIGN_DIR): mkdir(DATA_SIGN_DIR)\n",
    "  \n",
    "  with hands.Hands(\n",
    "    model_complexity=MP_MODEL_COMPLEXITY,\n",
    "    min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "    min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "    max_num_hands=1\n",
    "  ) as mp_hands:\n",
    "    for img_name in listdir(SIGN_DIR):\n",
    "      img_path = join(SIGN_DIR, img_name)\n",
    "      image = flip(imread(img_path), 1)\n",
    "      _, results = mediapipe_detection(image, mp_hands)\n",
    "      \n",
    "      if not results.multi_hand_landmarks:\n",
    "        print(f'unable to detect any hands for image: {img_name}')\n",
    "        reject(sign, img_name)\n",
    "        continue\n",
    "      else:\n",
    "        if len(results.multi_handedness) > 1:\n",
    "          print(f'detecting more than 1 hand for image: {img_name}')\n",
    "          reject(sign, img_name)\n",
    "          continue\n",
    "        else:\n",
    "          if get_handedness(results) != 'Right':\n",
    "            print(f'detected hand is not a Right hand for image {img_name}')\n",
    "            reject(sign, img_name)\n",
    "            continue\n",
    "    \n",
    "      keypoints = extract_keypoints_rh(results)\n",
    "      data_path = join(DATA_DIR, sign, img_name)\n",
    "      save(data_path, keypoints)  \n",
    "      accept(sign, img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! FOR TESTING, DO NOT RUN\n",
    "# for sign in SIGNS:\n",
    "#   SIGN_DIR = join(PROCCESSED_IMAGES_DIR, sign)\n",
    "  \n",
    "#   with hands.Hands(\n",
    "#     model_complexity=MP_MODEL_COMPLEXITY,\n",
    "#     min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "#     min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "#     max_num_hands=1\n",
    "#   ) as mp_hands:\n",
    "#     id = 0;\n",
    "#     for img_name in listdir(SIGN_DIR):\n",
    "#       img_path = join(SIGN_DIR, img_name)\n",
    "#       image = imread(img_path)\n",
    "#       results = mp_hands.process(cvtColor(image, COLOR_BGR2RGB))\n",
    "#       if results.multi_hand_landmarks:\n",
    "#         temp = image.copy()\n",
    "#         draw_img_landmarks(temp, results.multi_hand_landmarks)\n",
    "#         imwrite(join(IMAGES_DIR, 'temp', f'{img_name}.jpg'), temp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Load & Parition partition data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using 386 data points\n"
     ]
    }
   ],
   "source": [
    "def get_collection_count():\n",
    "  data_amounts = []\n",
    "  for collection_dir in listdir(DATA_DIR):\n",
    "    if collection_dir == '.gitkeep': continue\n",
    "    data_amounts.append(len(listdir(join(DATA_DIR, collection_dir))))\n",
    "      \n",
    "  return min(data_amounts)\n",
    "\n",
    "COLLECTION_COUNT = get_collection_count()\n",
    "print(f'currently using {COLLECTION_COUNT} data points')\n",
    "\n",
    "# Load Training Data\n",
    "label_map = { label: num for num, label in enumerate(ALL_SIGNS) }\n",
    "sequences, labels = [ # Initializing with 'none' sign\n",
    "  [\n",
    "    [\n",
    "      0 for i in range(HAND_LANDMARK_POINTS)\n",
    "    ] for j in range(SEQUENCE_LENGHT)\n",
    "  ] for k in range(COLLECTION_COUNT)\n",
    "], [\n",
    "  0 for i in range(COLLECTION_COUNT)\n",
    "]\n",
    "\n",
    "for sign in SIGNS:\n",
    "  sign_data_dir = join(DATA_DIR, sign)\n",
    "  for data_file_name in listdir(sign_data_dir)[:COLLECTION_COUNT]:\n",
    "    data_path = join(sign_data_dir, data_file_name)\n",
    "    res = load(data_path)\n",
    "    window = [res] * SEQUENCE_LENGHT\n",
    "    sequences.append(window)\n",
    "    labels.append(label_map[sign])\n",
    "\n",
    "x = array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "# Testing!\n",
    "s_expected = (CLASS_COUNT * COLLECTION_COUNT, SEQUENCE_LENGHT, HAND_LANDMARK_POINTS)\n",
    "s_result = x.shape\n",
    "l_expected = (CLASS_COUNT * COLLECTION_COUNT, CLASS_COUNT)\n",
    "l_result = y.shape\n",
    "if s_result != s_expected:\n",
    "  raise Exception(f'WARNING: expected sequence shape `{s_expected}` != from gotten `{s_result}`')\n",
    "if l_result != l_expected:\n",
    "  raise Exception(f'WARNING: expected labels shape `{l_expected}` != from gotten `{l_result}`')\n",
    "\n",
    "# partitioning train & test data\n",
    "x_train, x_text, y_train, y_test = train_test_split(x, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(MODEL_DIR)\n",
    "mkdir(LOG_DIR)\n",
    "tb_callback = TensorBoard(LOG_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24c25aa420167a70aa0145ae6ef574c07d08a593f4030ce527188d48dd10998d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
