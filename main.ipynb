{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os.path import join, realpath, dirname, exists, abspath, isfile, isdir\n",
    "from os import mkdir as mk, name as os_name, getcwd, environ, pathsep, rename, listdir\n",
    "from typing import Tuple\n",
    "\n",
    "from mediapipe.python.solutions import drawing_utils as du \n",
    "from mediapipe.python.solutions import hands\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.python.keras.utils.all_utils import to_categorical;\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import LSTM, Dense, Conv1D\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "from tensorflow.python.keras import optimizers\n",
    "import tensorflow as tf\n",
    "import tensorflowjs as tfjs\n",
    "\n",
    "from numpy import array, zeros, concatenate, save, load, argmax, expand_dims\n",
    "from uuid import uuid1\n",
    "\n",
    "import cv2\n",
    "from cv2 import imread, imshow, imwrite, flip, cvtColor, COLOR_BGR2RGB"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options 💾\n",
    "MODEL_NAME = 'v9_dan'\n",
    "\n",
    "MP_MODEL_COMPLEXITY = 0\n",
    "MP_DETECTION_CONFIDENCE = 0.75\n",
    "MP_TRACKING_CONFIDENCE = 0.75\n",
    "MP_NUM_HANDS = 1\n",
    "\n",
    "SIGNS = [\n",
    "  'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'\n",
    "]\n",
    "\n",
    "ALL_SIGNS = SIGNS.copy()\n",
    "ALL_SIGNS.insert(0, 'none')\n",
    "\n",
    "CLASS_COUNT = len(ALL_SIGNS)\n",
    "SEQUENCE_LENGHT = 10 # Amount of data per collection\n",
    "\n",
    "# Paths 📁\n",
    "ROOT_DIR = getcwd()\n",
    "MODELS_DIR = join(ROOT_DIR, 'models')\n",
    "MODEL_DIR = join(MODELS_DIR, MODEL_NAME)\n",
    "LOG_DIR = join(MODEL_DIR, 'logs')\n",
    "SAVED_MODEL_PATH = join(MODEL_DIR, 'signs.h5')\n",
    "EXPORTED_MODEL_DIR = join(MODEL_DIR)\n",
    "DATA_DIR = join(ROOT_DIR, 'data')\n",
    "IMAGES_DIR = join(ROOT_DIR, 'images')\n",
    "COLLECTED_IMAGES_DIR = join(IMAGES_DIR, 'collected')\n",
    "PREPROCESSED_IMAGES_DIR = join(IMAGES_DIR, 'preprocessed')\n",
    "PROCCESSED_IMAGES_DIR = join(IMAGES_DIR, 'processed')\n",
    "REJECTED_IMAGES_DIR = join(IMAGES_DIR, 'rejected')\n",
    "\n",
    "# Constants 🚧\n",
    "HAND_LANDMARK_COUNT = 21 # https://mediapipe.dev/images/mobile/hand_landmarks.png\n",
    "HAND_LANDMARK_POINTS = HAND_LANDMARK_COUNT * 3 # (x, y, z)\n",
    "\n",
    "# Util 📐\n",
    "def mkdir(path: str):\n",
    "  if not exists(path):\n",
    "    mk(path)\n",
    "  else:\n",
    "    print(f'{path} already exists!')\n",
    "\n",
    "def dir_exists(dir_path: str) -> bool:\n",
    "  return exists(dir_path) and isdir(dir_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mediapipe Util**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, results):\n",
    "  if not results.multi_hand_landmarks: return image\n",
    "  \n",
    "  hand_landmarks = results.multi_hand_landmarks\n",
    "  \n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(119, 252, 3), thickness=1, circle_radius=2), \n",
    "      du.DrawingSpec(color=(119, 252, 3), thickness=1, circle_radius=2)\n",
    "    )\n",
    "    \n",
    "def draw_img_landmarks(image, hand_landmarks):\n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(224,0,0), thickness=32, circle_radius=5), # points\n",
    "      du.DrawingSpec(color=(0,0,224), thickness=32, circle_radius=5) # edges\n",
    "    )\n",
    "\n",
    "def mediapipe_detection(image, hands: hands.Hands):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # COLOR CONVERSION BGR 2 RGB\n",
    "  image.flags.writeable = False                   # Image is no longer writeable\n",
    "  results = hands.process(image)                  # Make prediction\n",
    "  image.flags.writeable = True                    # Image is now writeable\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # COLOR COVERSION RGB 2 BGR\n",
    "  return image, results\n",
    "\n",
    "def extract_keypoints_rh(results):\n",
    "  if not results.multi_hand_landmarks:\n",
    "    return zeros(HAND_LANDMARK_POINTS)\n",
    "  \n",
    "  landmarks = MessageToDict(results.multi_hand_landmarks[0])['landmark']\n",
    "  res = []\n",
    "  \n",
    "  for lk in landmarks:\n",
    "    res.append(lk['x'])\n",
    "    res.append(lk['y'])\n",
    "    res.append(lk['z'])\n",
    "    \n",
    "  return array(res)\n",
    "\n",
    "def get_handedness(results):\n",
    "  return MessageToDict(results.multi_handedness[0])['classification'][0]['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Model Utils**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Model Topologies*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_0(input_shape: Tuple[int, int]) -> Sequential:\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=input_shape))\n",
    "  model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "  model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
    "  return model\n",
    "\n",
    "def model_1(input_shape: Tuple[int, int]) -> Sequential:\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(32, return_sequences=True, activation='relu', input_shape=input_shape))\n",
    "  model.add(LSTM(64, return_sequences=True, activation='relu'))\n",
    "  model.add(LSTM(128, return_sequences=False, activation='relu'))\n",
    "  model.add(Dense(128, activation='relu'))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
    "  return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Capture w/Mediapipe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "selected_sign_index = 0\n",
    "selected_sign = SIGNS[selected_sign_index]\n",
    "selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def select_next_sign():\n",
    "  global selected_sign, selected_sign_index, selected_sign_amount\n",
    "  if selected_sign_index < len(SIGNS) - 1:\n",
    "    selected_sign_index += 1\n",
    "    selected_sign = SIGNS[selected_sign_index]\n",
    "    selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def select_prev_sign():\n",
    "  global selected_sign, selected_sign_index, selected_sign_amount\n",
    "  if selected_sign_index > 0:\n",
    "    selected_sign_index -= 1\n",
    "    selected_sign = SIGNS[selected_sign_index]\n",
    "    selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "def capture(results):\n",
    "  global selected_sign, selected_sign_amount\n",
    "  if not results.multi_hand_landmarks:\n",
    "    print('no hand available')\n",
    "    return\n",
    "  if get_handedness(results) == 'Left':\n",
    "    print('must use right hand')\n",
    "    return\n",
    "\n",
    "  keypoints = extract_keypoints_rh(results)\n",
    "  data_path = join(DATA_DIR, selected_sign, f'{selected_sign}.{uuid1()}')\n",
    "  save(data_path, keypoints)\n",
    "  print(f'saved at {data_path}')\n",
    "  selected_sign_amount = len(listdir(join(DATA_DIR, selected_sign)))\n",
    "\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while cap.isOpened():\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    image = flip(image, 1)\n",
    "    \n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    image, results = mediapipe_detection(image, mp_hands)\n",
    "    draw_landmarks(image, results)\n",
    "    image = cv2.putText(\n",
    "      image, f'{selected_sign} | {selected_sign_amount}',\n",
    "      (32, 32), cv2.FONT_HERSHEY_SIMPLEX, 1, (119, 252, 3), 2, cv2.LINE_AA\n",
    "    )\n",
    "      \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    imshow('signs', image)\n",
    "    \n",
    "    key = cv2.waitKeyEx(1)\n",
    "\n",
    "    # if key != -1:\n",
    "    #   print(key)\n",
    "\n",
    "    if key == 32: # space bar\n",
    "      capture(results)\n",
    "\n",
    "    if key == 2555904: # right key\n",
    "      select_next_sign()\n",
    "\n",
    "    if key == 2424832: # left key\n",
    "      select_prev_sign()\n",
    "      \n",
    "    if key == 27:\n",
    "      break\n",
    "  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Renaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_folder_signs(_sign: str):\n",
    "  SOURCE_SIGN_DIR = join(COLLECTED_IMAGES_DIR, _sign)\n",
    "  DESTIN_SIGN_DIR = join(PREPROCESSED_IMAGES_DIR, _sign)\n",
    "  mkdir(DESTIN_SIGN_DIR)\n",
    "\n",
    "  if not exists(SOURCE_SIGN_DIR): raise Exception(f'make sure {SOURCE_SIGN_DIR} exists!')\n",
    "\n",
    "  for img_name in listdir(SOURCE_SIGN_DIR):\n",
    "    src_img_path = join(SOURCE_SIGN_DIR, img_name)\n",
    "    if isfile(src_img_path):\n",
    "      dest_img_path = join(DESTIN_SIGN_DIR, f'{_sign}.{uuid1()}.jpg')\n",
    "      rename(src_img_path, dest_img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL SIGNS\n",
    "for sign in SIGNS:\n",
    "  rename_folder_signs(sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SINGLE SIGN\n",
    "_SIGN = 'k'\n",
    "rename_folder_signs(_SIGN)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Data Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject(sign: str, img_name: str):\n",
    "  rename(\n",
    "    join(PREPROCESSED_IMAGES_DIR, sign, img_name),\n",
    "    join(REJECTED_IMAGES_DIR, img_name)\n",
    "  )\n",
    "  \n",
    "def accept(sign: str, img_name: str):\n",
    "  rename(\n",
    "    join(PREPROCESSED_IMAGES_DIR, sign, img_name),\n",
    "    join(PROCCESSED_IMAGES_DIR, img_name)\n",
    "  )\n",
    "\n",
    "def extract_data(sign: str):\n",
    "  SIGN_DIR = join(PREPROCESSED_IMAGES_DIR, sign)\n",
    "  DATA_SIGN_DIR = join(DATA_DIR, sign)\n",
    "  if not exists(DATA_SIGN_DIR): mkdir(DATA_SIGN_DIR)\n",
    "  \n",
    "  with hands.Hands(\n",
    "    model_complexity=MP_MODEL_COMPLEXITY,\n",
    "    min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "    min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "    max_num_hands=1\n",
    "  ) as mp_hands:\n",
    "    for img_name in listdir(SIGN_DIR):\n",
    "      img_path = join(SIGN_DIR, img_name)\n",
    "      image = flip(imread(img_path), 1)\n",
    "      _, results = mediapipe_detection(image, mp_hands)\n",
    "      \n",
    "      if not results.multi_hand_landmarks:\n",
    "        print(f'unable to detect any hands for image: {img_name}')\n",
    "        reject(sign, img_name)\n",
    "        continue\n",
    "      else:\n",
    "        if len(results.multi_handedness) > 1:\n",
    "          print(f'detecting more than 1 hand for image: {img_name}')\n",
    "          reject(sign, img_name)\n",
    "          continue\n",
    "        else:\n",
    "          if get_handedness(results) != 'Right':\n",
    "            print(f'detected hand is not a Right hand for image {img_name}')\n",
    "            reject(sign, img_name)\n",
    "            continue\n",
    "    \n",
    "      keypoints = extract_keypoints_rh(results)\n",
    "      data_path = join(DATA_DIR, sign, img_name)\n",
    "      save(data_path, keypoints)  \n",
    "      accept(sign, img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALL SIGNS\n",
    "for sign in SIGNS:\n",
    "  extract_data(sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SIGNLE SIGN\n",
    "extract_data('a')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Load & Parition partition data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_collection_count():\n",
    "  data_amounts = []\n",
    "  for collection_dir in listdir(DATA_DIR):\n",
    "    if collection_dir == '.gitkeep': continue\n",
    "    data_amounts.append(len(listdir(join(DATA_DIR, collection_dir))))\n",
    "      \n",
    "  return min(data_amounts)\n",
    "\n",
    "COLLECTION_COUNT = get_collection_count()\n",
    "print(f'currently using {COLLECTION_COUNT} data points')\n",
    "\n",
    "# Load Training Data\n",
    "label_map = { label: num for num, label in enumerate(ALL_SIGNS) }\n",
    "sequences, labels = [ # Initializing with 'none' sign\n",
    "  [\n",
    "    [\n",
    "      0 for i in range(HAND_LANDMARK_POINTS)\n",
    "    ] for j in range(SEQUENCE_LENGHT)\n",
    "  ] for k in range(COLLECTION_COUNT)\n",
    "], [\n",
    "  0 for i in range(COLLECTION_COUNT)\n",
    "]\n",
    "\n",
    "for sign in SIGNS:\n",
    "  sign_data_dir = join(DATA_DIR, sign)\n",
    "  for data_file_name in listdir(sign_data_dir)[:COLLECTION_COUNT]:\n",
    "    data_path = join(sign_data_dir, data_file_name)\n",
    "    res = load(data_path)\n",
    "    window = [res] * SEQUENCE_LENGHT\n",
    "    sequences.append(window)\n",
    "    labels.append(label_map[sign])\n",
    "\n",
    "x = array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "input_shape = (SEQUENCE_LENGHT, HAND_LANDMARK_POINTS)\n",
    "\n",
    "# Testing!\n",
    "s_expected = (CLASS_COUNT * COLLECTION_COUNT, SEQUENCE_LENGHT, HAND_LANDMARK_POINTS)\n",
    "s_result = x.shape\n",
    "l_expected = (CLASS_COUNT * COLLECTION_COUNT, CLASS_COUNT)\n",
    "l_result = y.shape\n",
    "if s_result != s_expected:\n",
    "  raise Exception(f'WARNING: expected sequence shape `{s_expected}` != from gotten `{s_result}`')\n",
    "if l_result != l_expected:\n",
    "  raise Exception(f'WARNING: expected labels shape `{l_expected}` != from gotten `{l_result}`')\n",
    "\n",
    "print(f'input shape is {input_shape}')\n",
    "\n",
    "# partitioning train, test, validation data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1 * 1.0)            # 10% test data (1.0 * 0.20 = 0.1 => 10%)\n",
    "x_train, x_val,  y_train, y_val  = train_test_split(x_train, y_train, test_size=0.1/0.9)  # 10% val  data (0.9 * 0.11 = 0.1 => 10%)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Training*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_model(): # Defining model to use\n",
    "  return model_1((SEQUENCE_LENGHT, HAND_LANDMARK_POINTS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir(MODEL_DIR)\n",
    "mkdir(LOG_DIR)\n",
    "tb_callback = TensorBoard(log_dir=LOG_DIR)\n",
    "terminal_log_dir = LOG_DIR.replace('\\\\', '/')\n",
    "\n",
    "model = get_model()\n",
    "model.compile(\n",
    "  optimizer=optimizers.adam_v2.Adam(learning_rate=0.0001),\n",
    "  loss='categorical_crossentropy',\n",
    "  metrics=['categorical_accuracy', 'categorical_crossentropy']\n",
    ")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "print(f'[RUN]: tensorboard --logdir={terminal_log_dir}')\n",
    "model.fit(\n",
    "  x_train, y_train, validation_data=(x_val, y_val),\n",
    "  epochs=300, batch_size=256,\n",
    "  use_multiprocessing=True, workers=4,\n",
    "  callbacks=[tb_callback],\n",
    "  steps_per_epoch=64,\n",
    "  shuffle=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(SAVED_MODEL_PATH)\n",
    "del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Test Model*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LOAD MODEL 💾\n",
    "model = get_model()\n",
    "model.load_weights(SAVED_MODEL_PATH)\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POST-TRAINING PREDICIONS ⚡\n",
    "res = model.predict(x_test)\n",
    "\n",
    "y_real = []\n",
    "y_pred = []\n",
    "\n",
    "for i in range(len(res)):\n",
    "  y_real.append(argmax(y_test[i]))\n",
    "  y_pred.append(argmax(res[i]))\n",
    "\n",
    "y_real = array(y_real)\n",
    "y_pred = array(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GENERATE CONFUSION MATRIX 😵\n",
    "cm = confusion_matrix(y_real, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=ALL_SIGNS)\n",
    "disp.plot(xticks_rotation=75)\n",
    "disp.figure_.set_size_inches(12.8, 7.2)\n",
    "disp.figure_.set_dpi(150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OVERALL ACC 🎯\n",
    "total = len(res)\n",
    "correct_preds = 0\n",
    "\n",
    "for i in range(total):\n",
    "  if y_real[i] == y_pred[i]:\n",
    "    correct_preds += 1\n",
    "\n",
    "acc_precentage = (correct_preds/total) * 100\n",
    "\n",
    "print(f'model {MODEL_NAME} has an overall accuracy of {\"{:.2f}\".format(acc_precentage)}%')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.load_weights(SAVED_MODEL_PATH)\n",
    "\n",
    "sequence = []\n",
    "predicted_sign = 'none'\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while cap.isOpened():\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    image = flip(image, 1)\n",
    "    \n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    image, results = mediapipe_detection(image, mp_hands)\n",
    "    draw_landmarks(image, results)\n",
    "    \n",
    "    keypoints = extract_keypoints_rh(results)\n",
    "    sequence.insert(0, keypoints)\n",
    "    sequence = sequence[:SEQUENCE_LENGHT]\n",
    "    \n",
    "    if len(sequence) == SEQUENCE_LENGHT:\n",
    "      output = model.predict(expand_dims(sequence, axis=0))[0]\n",
    "      res = argmax(output)\n",
    "      print(ALL_SIGNS[res], output[res])\n",
    "      \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    imshow('MediaPipe Hands', image)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "      break\n",
    "  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Export Model**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### *Topology*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "model.load_weights(SAVED_MODEL_PATH)\n",
    "tfjs.converters.save_keras_model(model, EXPORTED_MODEL_DIR)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "24c25aa420167a70aa0145ae6ef574c07d08a593f4030ce527188d48dd10998d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
