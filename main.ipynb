{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Imports**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "from os.path import join, realpath, dirname, exists, abspath, isfile, isdir\n",
    "from os import mkdir as mk, name as os_name, getcwd, environ, pathsep, rename, listdir\n",
    "from typing import Tuple\n",
    "\n",
    "from mediapipe.python.solutions import drawing_utils as du \n",
    "from mediapipe.python.solutions import hands\n",
    "from google.protobuf.json_format import MessageToDict\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.python.keras.utils.all_utils import to_categorical;\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import LSTM, Dense\n",
    "from tensorflow.python.keras.callbacks import TensorBoard\n",
    "from tensorflow.python.keras.backend import set_session\n",
    "import tensorflow as tf\n",
    "\n",
    "from numpy import array, zeros, concatenate, save, load, argmax\n",
    "from uuid import uuid1\n",
    "\n",
    "import cv2\n",
    "from cv2 import imread, imshow, imwrite, flip, cvtColor, COLOR_BGR2RGB\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Definitions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Options ðŸ’¾\n",
    "MODEL_NAME = 'v1'\n",
    "\n",
    "MP_MODEL_COMPLEXITY = 0\n",
    "MP_DETECTION_CONFIDENCE = 0.5\n",
    "MP_TRACKING_CONFIDENCE = 0.5\n",
    "MP_NUM_HANDS = 1\n",
    "\n",
    "SIGNS = [\n",
    "  'a', 'b', 'c',\n",
    "]\n",
    "\n",
    "ALL_SIGNS = SIGNS.copy()\n",
    "ALL_SIGNS.insert(0, 'none')\n",
    "\n",
    "CLASS_COUNT = len(ALL_SIGNS)\n",
    "SEQUENCE_LENGHT = 10 # Amount of data per collection\n",
    "\n",
    "# Paths ðŸ“\n",
    "ROOT_DIR = getcwd()\n",
    "MODELS_DIR = join(ROOT_DIR, 'models')\n",
    "MODEL_DIR = join(MODELS_DIR, MODEL_NAME)\n",
    "LOG_DIR = join(MODEL_DIR, 'logs')\n",
    "DATA_DIR = join(ROOT_DIR, 'data')\n",
    "IMAGES_DIR = join(ROOT_DIR, 'images')\n",
    "COLLECTED_IMAGES_DIR = join(IMAGES_DIR, 'collected')\n",
    "PREPROCESSED_IMAGES_DIR = join(IMAGES_DIR, 'preprocessed')\n",
    "PROCCESSED_IMAGES_DIR = join(IMAGES_DIR, 'processed')\n",
    "REJECTED_IMAGES_DIR = join(IMAGES_DIR, 'rejected')\n",
    "\n",
    "# Constants ðŸš§\n",
    "HAND_LANDMARK_COUNT = 21 # https://mediapipe.dev/images/mobile/hand_landmarks.png\n",
    "HAND_LANDMARK_POINTS = HAND_LANDMARK_COUNT * 3 # (x, y, z)\n",
    "\n",
    "# Util ðŸ“\n",
    "def mkdir(path: str):\n",
    "  if not exists(path):\n",
    "    mk(path)\n",
    "  else:\n",
    "    print(f'{path} already exists!')\n",
    "      \n",
    "def dir_exists(dir_path: str) -> bool:\n",
    "  return exists(dir_path) and isdir(dir_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Mediapipe Util**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_landmarks(image, hand_landmarks):\n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(121,22,76), thickness=2, circle_radius=4), \n",
    "      du.DrawingSpec(color=(121,44,250), thickness=2, circle_radius=2)\n",
    "    )\n",
    "    \n",
    "def draw_img_landmarks(image, hand_landmarks):\n",
    "  for point in hand_landmarks:\n",
    "    du.draw_landmarks(\n",
    "      image, point, hands.HAND_CONNECTIONS, \n",
    "      du.DrawingSpec(color=(224,0,0), thickness=32, circle_radius=5), # points\n",
    "      du.DrawingSpec(color=(0,0,224), thickness=32, circle_radius=5) # edges\n",
    "    )\n",
    "\n",
    "def mediapipe_detection(image, hands: hands.Hands):\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)  # COLOR CONVERSION BGR 2 RGB\n",
    "  image.flags.writeable = False                   # Image is no longer writeable\n",
    "  results = hands.process(image)                  # Make prediction\n",
    "  image.flags.writeable = True                    # Image is now writeable\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)  # COLOR COVERSION RGB 2 BGR\n",
    "  return image, results\n",
    "\n",
    "def extract_keypoints_rh(results):\n",
    "  landmarks = MessageToDict(results.multi_hand_landmarks[0])['landmark']\n",
    "  res = []\n",
    "  \n",
    "  for lk in landmarks:\n",
    "    res.append(lk['x'])\n",
    "    res.append(lk['y'])\n",
    "    res.append(lk['z'])\n",
    "    \n",
    "  return array(res)\n",
    "\n",
    "def get_handedness(results):\n",
    "  return MessageToDict(results.multi_handedness[0])['classification'][0]['label']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### *Model Structures*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_0(input_shape: Tuple[int, int]) -> Sequential:\n",
    "  model = Sequential()\n",
    "  model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=input_shape))\n",
    "  model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "  model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
    "  return model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Capture w/Mediapipe**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0)\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while cap.isOpened():\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    image = flip(image, 1)\n",
    "    \n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    image, results = mediapipe_detection(image, mp_hands)\n",
    "    if results.multi_hand_landmarks:\n",
    "      draw_landmarks(image, results.multi_hand_landmarks)\n",
    "      \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    imshow('MediaPipe Hands', image)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "      break\n",
    "  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Renaming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "_SIGN = 'c'\n",
    "SOURCE_SIGN_DIR = join(COLLECTED_IMAGES_DIR, _SIGN)\n",
    "DESTIN_SIGN_DIR = join(PREPROCESSED_IMAGES_DIR, _SIGN)\n",
    "mkdir(DESTIN_SIGN_DIR)\n",
    "\n",
    "if not exists(SOURCE_SIGN_DIR): raise Exception(f'make sure {SOURCE_SIGN_DIR} exists!')\n",
    "\n",
    "for img_name in listdir(SOURCE_SIGN_DIR):\n",
    "  src_img_path = join(SOURCE_SIGN_DIR, img_name)\n",
    "  if isfile(src_img_path):\n",
    "    dest_img_path = join(DESTIN_SIGN_DIR, f'{_SIGN}.{uuid1()}.jpg')\n",
    "    rename(src_img_path, dest_img_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Image Data Extraction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reject(sign: str, img_name: str):\n",
    "  rename(\n",
    "    join(PREPROCESSED_IMAGES_DIR, sign, img_name),\n",
    "    join(REJECTED_IMAGES_DIR, img_name)\n",
    "  )\n",
    "  \n",
    "def accept(sign: str, img_name: str):\n",
    "  rename(\n",
    "    join(PREPROCESSED_IMAGES_DIR, sign, img_name),\n",
    "    join(PROCCESSED_IMAGES_DIR, img_name)\n",
    "  )\n",
    "\n",
    "for sign in SIGNS:\n",
    "  SIGN_DIR = join(PREPROCESSED_IMAGES_DIR, sign)\n",
    "  DATA_SIGN_DIR = join(DATA_DIR, sign)\n",
    "  if not exists(DATA_SIGN_DIR): mkdir(DATA_SIGN_DIR)\n",
    "  \n",
    "  with hands.Hands(\n",
    "    model_complexity=MP_MODEL_COMPLEXITY,\n",
    "    min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "    min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "    max_num_hands=1\n",
    "  ) as mp_hands:\n",
    "    for img_name in listdir(SIGN_DIR):\n",
    "      img_path = join(SIGN_DIR, img_name)\n",
    "      image = flip(imread(img_path), 1)\n",
    "      _, results = mediapipe_detection(image, mp_hands)\n",
    "      \n",
    "      if not results.multi_hand_landmarks:\n",
    "        print(f'unable to detect any hands for image: {img_name}')\n",
    "        reject(sign, img_name)\n",
    "        continue\n",
    "      else:\n",
    "        if len(results.multi_handedness) > 1:\n",
    "          print(f'detecting more than 1 hand for image: {img_name}')\n",
    "          reject(sign, img_name)\n",
    "          continue\n",
    "        else:\n",
    "          if get_handedness(results) != 'Right':\n",
    "            print(f'detected hand is not a Right hand for image {img_name}')\n",
    "            reject(sign, img_name)\n",
    "            continue\n",
    "    \n",
    "      keypoints = extract_keypoints_rh(results)\n",
    "      data_path = join(DATA_DIR, sign, img_name)\n",
    "      save(data_path, keypoints)  \n",
    "      accept(sign, img_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! FOR TESTING, DO NOT RUN\n",
    "# for sign in SIGNS:\n",
    "#   SIGN_DIR = join(PROCCESSED_IMAGES_DIR, sign)\n",
    "  \n",
    "#   with hands.Hands(\n",
    "#     model_complexity=MP_MODEL_COMPLEXITY,\n",
    "#     min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "#     min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "#     max_num_hands=1\n",
    "#   ) as mp_hands:\n",
    "#     id = 0;\n",
    "#     for img_name in listdir(SIGN_DIR):\n",
    "#       img_path = join(SIGN_DIR, img_name)\n",
    "#       image = imread(img_path)\n",
    "#       results = mp_hands.process(cvtColor(image, COLOR_BGR2RGB))\n",
    "#       if results.multi_hand_landmarks:\n",
    "#         temp = image.copy()\n",
    "#         draw_img_landmarks(temp, results.multi_hand_landmarks)\n",
    "#         imwrite(join(IMAGES_DIR, 'temp', f'{img_name}.jpg'), temp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Train**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### *Load & Parition partition data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "currently using 386 data points\n",
      "input shape is (10, 63)\n"
     ]
    }
   ],
   "source": [
    "def get_collection_count():\n",
    "  data_amounts = []\n",
    "  for collection_dir in listdir(DATA_DIR):\n",
    "    if collection_dir == '.gitkeep': continue\n",
    "    data_amounts.append(len(listdir(join(DATA_DIR, collection_dir))))\n",
    "      \n",
    "  return min(data_amounts)\n",
    "\n",
    "COLLECTION_COUNT = get_collection_count()\n",
    "print(f'currently using {COLLECTION_COUNT} data points')\n",
    "\n",
    "# Load Training Data\n",
    "label_map = { label: num for num, label in enumerate(ALL_SIGNS) }\n",
    "sequences, labels = [ # Initializing with 'none' sign\n",
    "  [\n",
    "    [\n",
    "      0 for i in range(HAND_LANDMARK_POINTS)\n",
    "    ] for j in range(SEQUENCE_LENGHT)\n",
    "  ] for k in range(COLLECTION_COUNT)\n",
    "], [\n",
    "  0 for i in range(COLLECTION_COUNT)\n",
    "]\n",
    "\n",
    "for sign in SIGNS:\n",
    "  sign_data_dir = join(DATA_DIR, sign)\n",
    "  for data_file_name in listdir(sign_data_dir)[:COLLECTION_COUNT]:\n",
    "    data_path = join(sign_data_dir, data_file_name)\n",
    "    res = load(data_path)\n",
    "    window = [res] * SEQUENCE_LENGHT\n",
    "    sequences.append(window)\n",
    "    labels.append(label_map[sign])\n",
    "\n",
    "x = array(sequences)\n",
    "y = to_categorical(labels).astype(int)\n",
    "\n",
    "input_shape = (SEQUENCE_LENGHT, HAND_LANDMARK_POINTS)\n",
    "\n",
    "# Testing!\n",
    "s_expected = (CLASS_COUNT * COLLECTION_COUNT, SEQUENCE_LENGHT, HAND_LANDMARK_POINTS)\n",
    "s_result = x.shape\n",
    "l_expected = (CLASS_COUNT * COLLECTION_COUNT, CLASS_COUNT)\n",
    "l_result = y.shape\n",
    "if s_result != s_expected:\n",
    "  raise Exception(f'WARNING: expected sequence shape `{s_expected}` != from gotten `{s_result}`')\n",
    "if l_result != l_expected:\n",
    "  raise Exception(f'WARNING: expected labels shape `{l_expected}` != from gotten `{l_result}`')\n",
    "\n",
    "print(f'input shape is {input_shape}')\n",
    "# partitioning train & test data\n",
    "x_train, x_text, y_train, y_test = train_test_split(x, y, test_size=0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logging for TB\n",
    "mkdir(MODEL_DIR)\n",
    "mkdir(LOG_DIR)\n",
    "tb_callback = TensorBoard(log_dir=LOG_DIR)\n",
    "\n",
    "\n",
    "tf_config = tf.compat.v1.ConfigProto(allow_soft_placement=False)\n",
    "tf_config.gpu_options.allow_growth = True\n",
    "s = tf.compat.v1.Session(config=tf_config)\n",
    "set_session(s)\n",
    "\n",
    "\n",
    "with tf.device('/gpu:0'):\n",
    "  # model = model_0(input_shape)\n",
    "  model = Sequential()\n",
    "\n",
    "  model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=input_shape))\n",
    "  model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "  model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "  model.add(Dense(64, activation='relu'))\n",
    "  model.add(Dense(32, activation='relu'))\n",
    "  model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
    "\n",
    "  model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['categorical_accuracy'])\n",
    "\n",
    "  print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "  print(f'[RUN]: tensorboard --logdir={LOG_DIR}')\n",
    "  model.fit(x_train, y_train, epochs=100, use_multiprocessing=True, workers=4, batch_size=2048)\n",
    "\n",
    "  model.save(join(MODEL_DIR, 'signs.h5'))\n",
    "  del model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[ WARN:0@3710.638] global cap_v4l.cpp:969 open VIDEOIO(V4L2): can't find camera device\n",
      "[ERROR:0@3710.638] global obsensor_uvc_stream_channel.cpp:156 getStreamChannelGroup Camera index out of range\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(LSTM(64, return_sequences=True, activation='relu', input_shape=input_shape))\n",
    "model.add(LSTM(128, return_sequences=True, activation='relu'))\n",
    "model.add(LSTM(64, return_sequences=False, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dense(CLASS_COUNT, activation='softmax'))\n",
    "model.load_weights(join(MODEL_DIR, 'signs.h5'))\n",
    "\n",
    "sequence = []\n",
    "sentence = []\n",
    "threshold = 0.5\n",
    "\n",
    "cap = cv2.VideoCapture(1)\n",
    "with hands.Hands(\n",
    "  model_complexity=MP_MODEL_COMPLEXITY,\n",
    "  min_detection_confidence=MP_DETECTION_CONFIDENCE,\n",
    "  min_tracking_confidence=MP_TRACKING_CONFIDENCE,\n",
    "  max_num_hands=MP_NUM_HANDS\n",
    ") as mp_hands:\n",
    "  while cap.isOpened():\n",
    "    \n",
    "    success, image = cap.read()\n",
    "    image = flip(image, 1)\n",
    "    \n",
    "    if not success:\n",
    "      print(\"Ignoring empty camera frame.\")\n",
    "      continue\n",
    "\n",
    "    image, results = mediapipe_detection(image, mp_hands)\n",
    "    \n",
    "    if results.multi_hand_landmarks:\n",
    "      draw_landmarks(image, results.multi_hand_landmarks)\n",
    "      \n",
    "    # Flip the image horizontally for a selfie-view display.\n",
    "    imshow('MediaPipe Hands', image)\n",
    "    \n",
    "    key = cv2.waitKey(1)\n",
    "    if key == ord('q'):\n",
    "      break\n",
    "  \n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prediciton: b, real: c\n"
     ]
    }
   ],
   "source": [
    "res = model.predict(x_text)\n",
    "\n",
    "for i in range(len(res)):\n",
    "  pre = ALL_SIGNS[argmax(res[i])]\n",
    "  rel = ALL_SIGNS[argmax(y_test[i])]\n",
    "  if rel != pre:\n",
    "    print(f'prediciton: {pre}, real: {rel}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lsv-mp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c409e91ec21a45890178908fef7c1db143f872571efb8e413526c800a51880a3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
